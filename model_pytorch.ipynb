{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bs4 as bs\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('AOL-user-ct-collection/user-ct-test-collection-01.txt') as file:\n",
    "    data_aol = [line.strip().split('\\t') for line in file.readlines()][1:]\n",
    "    \n",
    "with open('AOL-user-ct-collection/user-ct-test-collection-02.txt') as file:\n",
    "    data_aol += [line.strip().split('\\t') for line in file.readlines()][1:]\n",
    "    \n",
    "with open('AOL-user-ct-collection/user-ct-test-collection-03.txt') as file:\n",
    "    data_aol += [line.strip().split('\\t') for line in file.readlines()][1:]\n",
    "    \n",
    "with open('AOL-user-ct-collection/user-ct-test-collection-04.txt') as file:\n",
    "    data_aol += [line.strip().split('\\t') for line in file.readlines()][1:]\n",
    "    \n",
    "with open('AOL-user-ct-collection/user-ct-test-collection-05.txt') as file:\n",
    "    data_aol += [line.strip().split('\\t') for line in file.readlines()][1:]\n",
    "    \n",
    "with open('AOL-user-ct-collection/user-ct-test-collection-06.txt') as file:\n",
    "    data_aol += [line.strip().split('\\t') for line in file.readlines()][1:]\n",
    "    \n",
    "with open('AOL-user-ct-collection/user-ct-test-collection-07.txt') as file:\n",
    "    data_aol += [line.strip().split('\\t') for line in file.readlines()][1:]\n",
    "    \n",
    "with open('AOL-user-ct-collection/user-ct-test-collection-08.txt') as file:\n",
    "    data_aol += [line.strip().split('\\t') for line in file.readlines()][1:]\n",
    "    \n",
    "with open('AOL-user-ct-collection/user-ct-test-collection-09.txt') as file:\n",
    "    data_aol += [line.strip().split('\\t') for line in file.readlines()][1:]\n",
    "    \n",
    "with open('AOL-user-ct-collection/user-ct-test-collection-10.txt') as file:\n",
    "    data_aol_test = [line.strip().split('\\t') for line in file.readlines()][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на уникальное количество пользователей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique users in train = 591773, Count of unique users in test = 65654\n"
     ]
    }
   ],
   "source": [
    "unique_users_train, count_users_train = np.unique([line[0] for line in data_aol], return_counts=True)\n",
    "unique_users_test, count_users_test = np.unique([line[0] for line in data_aol_test], return_counts=True)\n",
    "print('Count of unique users in train = {}, Count of unique users in test = {}'.format(len(unique_users_train),\n",
    "                                                                                       len(unique_users_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на пересечения юзеров между трейном и тестом. Пересечение пустое. Остюда можно сделать вывод, что разбиение на тест и трейн корректно. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype='<U8')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(unique_users_train, unique_users_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токинизируем запросы при помощи WordPunctTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "for i in range(1, len(data_aol)):\n",
    "    data_aol[i][1] = ' '.join(tokenizer.tokenize(data_aol[i][1].lower()))\n",
    "    dtime = datetime.strptime(data_aol[i][2], \"%Y-%m-%d %H:%M:%S\")\n",
    "    data_aol[i][2] = time.mktime(dtime.timetuple())\n",
    "\n",
    "for i in range(1, len(data_aol_test)):\n",
    "    data_aol_test[i][1] = ' '.join(tokenizer.tokenize(data_aol_test[i][1].lower()))\n",
    "    dtime = datetime.strptime(data_aol_test[i][2], \"%Y-%m-%d %H:%M:%S\")\n",
    "    data_aol_test[i][2] = time.mktime(dtime.timetuple())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save/load\n",
    "\n",
    "# import pickle \n",
    "\n",
    "# with open('data_aol.pickle', 'wb') as f:\n",
    "#     pickle.dump(data_aol, f)\n",
    "\n",
    "# with open('data_aol_test.pickle', 'wb') as f:\n",
    "#     pickle.dump(data_aol_test, f)\n",
    "    \n",
    "# with open('data_aol.pickle', 'rb') as f:\n",
    "#     data_aol = pickle.load(f)\n",
    "        \n",
    "# with open('data_aol_test.pickle', 'rb') as f:\n",
    "#     data_aol_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша будущая модель будет предсказывать продолжение запроса по уже написанной части предсказываемого запроса и по текущей сессии. Под текущей сессией я решила понимать запросы, расположенные друг от друга не больше, чем на пол часа. При этом я рассматриваю только максимум 6 предыдущих запросов. <br>\n",
    "Таким образом, в качестве одного объекта, который будем подавать в модель, нужно взять последовательность запросов (не больше 6) из одной сессии одного юзера. При этом мы не хотим брать объект вида один запрос. Найдем подходящие индексы, чтобы потом быстрее семплить при построении батча."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_query_sequence_len = 6\n",
    "session_time = 30 * 60 # 30 minutes\n",
    "\n",
    "count_prev_good_indeces = [0]\n",
    "prev_time = data_aol[0][2]\n",
    "prev_user_id = data_aol[0][0]\n",
    "\n",
    "cur_good_count = 1\n",
    "for i in range(1, len(data_aol)):\n",
    "    if data_aol[i][2] - prev_time < session_time and data_aol[i][0] == prev_user_id:\n",
    "        count_prev_good_indeces.append(cur_good_count)\n",
    "        cur_good_count += 1\n",
    "    else:\n",
    "        count_prev_good_indeces.append(0)\n",
    "        cur_good_count = 1\n",
    "    prev_user_id = data_aol[i][0]\n",
    "    prev_time = data_aol[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_prev_good_indeces = np.array(count_prev_good_indeces)\n",
    "good_last_indeces_for_batch = np.where(count_prev_good_indeces != 0)[0]\n",
    "\n",
    "count_prev_good_indeces_limited = np.min((count_prev_good_indeces + 1, \n",
    "                                          np.array([max_query_sequence_len] * len(data_aol))), axis=0)\n",
    "count_prev_good_indeces_limited  = count_prev_good_indeces + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое сделаю и для теста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "\n",
    "count_prev_good_indeces_test = [0]\n",
    "prev_time = data_aol_test[0][2]\n",
    "prev_user_id = data_aol_test[0][0]\n",
    "\n",
    "cur_good_count = 1\n",
    "for i in range(1, len(data_aol_test)):\n",
    "    if data_aol_test[i][2] - prev_time < session_time and data_aol_test[i][0] == prev_user_id:\n",
    "        count_prev_good_indeces_test.append(cur_good_count)\n",
    "        cur_good_count += 1\n",
    "    else:\n",
    "        count_prev_good_indeces_test.append(0)\n",
    "        cur_good_count = 1\n",
    "    prev_user_id = data_aol_test[i][0]\n",
    "    prev_time = data_aol_test[i][2]\n",
    "    \n",
    "count_prev_good_indeces_test = np.array(count_prev_good_indeces_test)\n",
    "good_last_indeces_for_batch_test = np.where(count_prev_good_indeces_test != 0)[0]\n",
    "\n",
    "count_prev_good_indeces_limited_test = np.min((count_prev_good_indeces_test + 1, \n",
    "                                          np.array([max_query_sequence_len] * len(data_aol_test))), axis=0)\n",
    "count_prev_good_indeces_limited_test  = count_prev_good_indeces_test + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процентное соотношение хороших индексов ко всем. Под хорошими индексами понимаются индексы запросов, перед которыми есть хотя бы какое-то количество запросов в текущей сессии. Остальные запросы нам нет смысла рассматривать, так как мы решаем задачу персонализированных предсказаний. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6995085727871232"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(count_prev_good_indeces) != 0).sum() / len(count_prev_good_indeces)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уникальное количество слов во всех запросах просто огромное : превышает 3 миллиона. Поэтому мы, конечно, можем скачать уже готовую модель эмбеддингов и решать по сути задачу классификации на \"размер словаря\" классов, но для этого потребуются какие-то невероятные вычислительные мощности. Поэтому вместо обычных слов будем использовать BPE-токены. BPE-токены частые слова оставят неизменными, а редкие слова разобъют на несколько подслов, которые уже более часто встречаются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading embedding model\n",
    "# import gensim.downloader \n",
    "# gensim.downloader.info()['models'].keys() \n",
    "# model_wv = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "\n",
    "# split and tokenize the data\n",
    "with open('train.en', 'w') as f_src:\n",
    "    for line in data_aol:\n",
    "        f_src.write(line[1] + '\\n')\n",
    "    for line in data_aol_test:\n",
    "        f_src.write(line[1] + '\\n')\n",
    "    \n",
    "# build and apply bpe vocs\n",
    "learn_bpe(open('./train.en'), open('bpe_rules.en', 'w'), num_symbols=40000)\n",
    "bpe = BPE(open('./bpe_rules.en'))\n",
    "\n",
    "with open('train.bpe.en', 'w') as f_out:\n",
    "    for line in open('train.en'):\n",
    "        f_out.write(bpe.process_line(line.strip()) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перепишем весь наш датасет в терминах BPE токенов. Теперь у нас появились словосочетания типа rent@@ direct, что означает, что мы разбили слово rentdirect на 2 более частых токена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_aol)):\n",
    "    data_aol[i][1] = bpe.process_line(data_aol[i][1].strip())\n",
    "for i in range(len(data_aol_test)):\n",
    "    data_aol_test[i][1] = bpe.process_line(data_aol_test[i][1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['142', 'rent@@ direct . com', 1141186632.0]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_aol[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# with open('data_aol_bpe.pickle', 'wb') as f:\n",
    "#     pickle.dump(data_aol, f)\n",
    "\n",
    "# with open('data_aol_test_bpe.pickle', 'wb') as f:\n",
    "#     pickle.dump(data_aol_test, f)\n",
    "    \n",
    "with open('data_aol_bpe.pickle', 'rb') as f:\n",
    "    data_aol = pickle.load(f)\n",
    "        \n",
    "with open('data_aol_test_bpe.pickle', 'rb') as f:\n",
    "    data_aol_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем кол-во уникальных слов. К токенам также добавим токены BOS (begin of sentence), EOS (end if sentence), UNK (unknown), PAD (padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set(' '.join((open('./train.bpe.en').read().split('\\n'))).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words.add('BOS')\n",
    "all_words.add('EOS')\n",
    "all_words.add('UNK')\n",
    "all_words.add('PAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50148"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составим отображение из слов в индексы и обратно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_indeces = dict(zip(list(all_words), np.arange(len(all_words))))\n",
    "indeces_to_words = dict(zip(np.arange(len(all_words)), list(all_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем/считываем все данные, полученные ранее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('data_aol_tr.pickle', 'wb') as f:\n",
    "#     pickle.dump(data_aol, f)\n",
    "\n",
    "# with open('good_last_indeces_for_batch_tr.pickle', 'wb') as f:\n",
    "#     pickle.dump(good_last_indeces_for_batch, f)\n",
    "    \n",
    "# with open('data_aol_test_tr.pickle', 'wb') as f:\n",
    "#     pickle.dump(data_aol_test, f)\n",
    "\n",
    "# with open('good_last_indeces_for_batch_test_tr.pickle', 'wb') as f:\n",
    "#     pickle.dump(good_last_indeces_for_batch_test, f)\n",
    "    \n",
    "# with open('count_prev_good_indeces_limited_test_tr.pickle', 'wb') as f:\n",
    "#     pickle.dump(count_prev_good_indeces_limited_test, f)\n",
    "    \n",
    "# with open('words_to_indeces_tr.pickle', 'wb') as f:\n",
    "#     pickle.dump(words_to_indeces, f)\n",
    "    \n",
    "# with open('indeces_to_words_tr.pickle', 'wb') as f:\n",
    "#     pickle.dump(indeces_to_words, f)\n",
    "    \n",
    "# with open('all_words_tr.pickle', 'wb') as f:\n",
    "#     pickle.dump(all_words, f)\n",
    "    \n",
    "# with open('count_prev_good_indeces_limited_tr.pickle', 'wb') as f:\n",
    "#     pickle.dump(count_prev_good_indeces_limited, f)\n",
    "    \n",
    "\n",
    "with open('data_aol_tr.pickle', 'rb') as f:\n",
    "    data_aol = pickle.load(f)\n",
    "\n",
    "with open('good_last_indeces_for_batch_tr.pickle', 'rb') as f:\n",
    "    good_last_indeces_for_batch = pickle.load(f)\n",
    "    \n",
    "with open('data_aol_test_tr.pickle', 'rb') as f:\n",
    "    data_aol_test = pickle.load(f)\n",
    "\n",
    "with open('good_last_indeces_for_batch_test_tr.pickle', 'rb') as f:\n",
    "    good_last_indeces_for_batch_test = pickle.load(f)\n",
    "    \n",
    "with open('count_prev_good_indeces_limited_test_tr.pickle', 'rb') as f:\n",
    "    count_prev_good_indeces_limited_test = pickle.load(f)\n",
    "    \n",
    "with open('words_to_indeces_tr.pickle', 'rb') as f:\n",
    "    words_to_indeces = pickle.load(f)\n",
    "    \n",
    "with open('indeces_to_words_tr.pickle', 'rb') as f:\n",
    "    indeces_to_words = pickle.load(f)\n",
    "    \n",
    "with open('all_words_tr.pickle', 'rb') as f:\n",
    "    all_words = pickle.load(f)\n",
    "    \n",
    "with open('count_prev_good_indeces_limited_tr.pickle', 'rb') as f:\n",
    "    count_prev_good_indeces_limited = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация батча"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для генерации батча. В качестве одного объекта нужно взять последовательность запросов (не больше 6) из одной сессии одного юзера. Здесь же превращаем все эти запросы в матричку, которую уже принимает на вход модель.\n",
    "\n",
    "Также напишем функцию, возвращающую объекты, необходимые модели для предсказания : get_data_for_prediction. Эта функция возвращает матрицы, соответствующие предыдущим запросам сессии, матрицу, соответствующую началу предсказываемого запроса и настоящий предсказываемый запрос, чтобы потом было удобно сравниваться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameters\n",
    "vocab_size = len(all_words)\n",
    "emb_size = 300\n",
    "batch_size = 15\n",
    "max_query_sequence_len = 6\n",
    "session_time = 30 * 60 # 30 minutes\n",
    "max_word_count_without_eos = 8\n",
    "max_word_count = max_word_count_without_eos + 1\n",
    "\n",
    "eos_ix = words_to_indeces['EOS']\n",
    "bos_ix = words_to_indeces['BOS']\n",
    "pad_ix = words_to_indeces['PAD']\n",
    "unk_ix = words_to_indeces['UNK']\n",
    "\n",
    "# embedding for BOS - np.ones(emb_size)\n",
    "# embedding for EOS - np.zeros(emb_size)\n",
    "\n",
    "# empty_query_emb = np.vstack((np.ones((1, emb_size)), np.zeros((max_word_count - 1, emb_size))))[None]\n",
    "\n",
    "# empty_query_emb = np.zeros((max_word_count - 1, emb_size))[None]\n",
    "# empty_query_target = np.array([bos_ix] + [eos_ix] * (max_word_count - 1))[None]\n",
    "\n",
    "# empty_query_emb = np.array([bos_ix] + [eos_ix] * (max_word_count_without_eos - 1))[None]\n",
    "empty_query_emb = np.array([pad_ix] * (max_word_count))[None]\n",
    "\n",
    "def data_to_matrices(data, good_last_indeces, count_prev_good_indeces_limited, \n",
    "                     emb_size=300, min_count_prev_queries=1, max_batch_size=136):\n",
    "    prev_counts = count_prev_good_indeces_limited[good_last_indeces]\n",
    "    begin_indeces = good_last_indeces - prev_counts + 1\n",
    "\n",
    "    new_data = []\n",
    "    \n",
    "    for i in range(len(begin_indeces)):\n",
    "        query_sequence = [data[j][1] for j in range(begin_indeces[i], good_last_indeces[i] + 1)]\n",
    "        current_sequence_of_queries = []\n",
    "        prev_line = ''\n",
    "        for line in query_sequence:\n",
    "            #delete repeating queries\n",
    "            if line == prev_line:\n",
    "                continue\n",
    "            if len(current_sequence_of_queries) == max_query_sequence_len:\n",
    "                break\n",
    "            \n",
    "            prev_line = line\n",
    "            query_words = []\n",
    "#             query_words.append(np.ones(emb_size)) # BOS word\n",
    "            query_targets = []\n",
    "            for word in line.split()[:max_word_count_without_eos]:\n",
    "\n",
    "                if word in all_words:\n",
    "                    query_words.append(words_to_indeces[word])\n",
    "                else:\n",
    "                    query_words.append(unk_ix)\n",
    "                    \n",
    "            query_words.append(eos_ix)\n",
    "            count_additional_words = max_word_count - len(query_words)\n",
    "            if count_additional_words > 0:\n",
    "                query_words += list(np.ones(count_additional_words).astype(int) * pad_ix)\n",
    "            # add EOS word at the end of sentence\n",
    "#             query_targets.append(eos_ix)\n",
    "            current_sequence_of_queries.append(np.array(query_words))\n",
    "        if len(current_sequence_of_queries) <= min_count_prev_queries:\n",
    "            continue\n",
    "        count_additional_queries = max_query_sequence_len - len(current_sequence_of_queries)\n",
    "        if (count_additional_queries > 0):\n",
    "#             current_sequence_of_queries = list(np.tile(empty_query_emb, reps=(count_additional_queries, 1, 1))) + current_sequence_of_queries\n",
    "            current_sequence_of_queries = list(np.tile(empty_query_emb, reps=(count_additional_queries, 1))) + current_sequence_of_queries\n",
    "\n",
    "            \n",
    "        assert len(current_sequence_of_queries) == max_query_sequence_len\n",
    "        new_data.append(current_sequence_of_queries)\n",
    "\n",
    "    return np.array(new_data)[:max_batch_size]\n",
    "\n",
    "\n",
    "def test_queries_to_matrices(batch_prev_queries, batch_next_queries, count_words, return_answers=False):\n",
    "    \n",
    "    #prev_queries to matrix\n",
    "    prev_queries_matrix = []\n",
    "    for prev_queries in batch_prev_queries:\n",
    "        current_sequence_of_queries = []\n",
    "        for query in prev_queries[-(max_query_sequence_len - 1):]:\n",
    "            query_words = []\n",
    "            for word in query.split()[:max_word_count_without_eos]:\n",
    "                if word in all_words:\n",
    "                    query_words.append(words_to_indeces[word])\n",
    "                else:\n",
    "                    query_words.append(unk_ix)\n",
    "                    \n",
    "            query_words.append(eos_ix)\n",
    "            count_additional_words = max_word_count - len(query_words)\n",
    "            if count_additional_words > 0:\n",
    "                query_words += list(np.ones(count_additional_words).astype(int) * pad_ix)\n",
    "            current_sequence_of_queries.append(np.array(query_words))\n",
    "            \n",
    "        count_additional_queries = max_query_sequence_len - len(current_sequence_of_queries) - 1\n",
    "        if (count_additional_queries > 0):\n",
    "            current_sequence_of_queries = list(np.tile(empty_query_emb, reps=(count_additional_queries, 1))) + current_sequence_of_queries\n",
    "        prev_queries_matrix.append(current_sequence_of_queries)\n",
    "    \n",
    "    #next_queries to matrix\n",
    "    next_queries_matrix = []\n",
    "    for next_query in batch_next_queries:\n",
    "        query_words = []\n",
    "    \n",
    "        for word in next_query.split()[:count_words]:\n",
    "            if word in all_words:\n",
    "                query_words.append(words_to_indeces[word])\n",
    "            else:\n",
    "                query_words.append(unk_ix)\n",
    "        count_additional_words = count_words - len(query_words)\n",
    "        if count_additional_words > 0:\n",
    "            query_words += list(np.ones(count_additional_words).astype(int) * pad_ix)\n",
    "        next_queries_matrix.append(query_words)\n",
    "        \n",
    "    if return_answers:\n",
    "        return np.array(prev_queries_matrix), np.array(next_queries_matrix), batch_next_queries\n",
    "    else:\n",
    "        return np.array(prev_queries_matrix), np.array(next_queries_matrix), None\n",
    "\n",
    "    \n",
    "def get_one_batch(data, good_last_indeces_for_batch, batch_size=32, emb_size=300):\n",
    "    batch_end_indeces = np.random.choice(good_last_indeces_for_batch, batch_size, replace=False)\n",
    "    \n",
    "    return data_to_matrices(data, batch_end_indeces, emb_size)\n",
    "\n",
    "\n",
    "def get_batch_generator(data, good_last_indeces_for_batch, batch_size=32, emb_size=300, \n",
    "                        min_count_prev_queries=1, max_batch_size=136):\n",
    "    while True:\n",
    "        \n",
    "        batch_end_indeces = np.random.choice(good_last_indeces_for_batch, batch_size, replace=False)\n",
    "    \n",
    "        yield data_to_matrices(data, batch_end_indeces, count_prev_good_indeces_limited, \n",
    "                               emb_size, min_count_prev_queries, max_batch_size)\n",
    "        \n",
    "\n",
    "def get_val_batch_generator(data, good_last_indeces, count_prev_good_indeces_limited,\n",
    "                            batch_size=32, emb_size=300):\n",
    "    indices = np.arange(len(good_last_indeces))\n",
    "    \n",
    "    for start in range(0, len(indices), batch_size):\n",
    "        ix = indices[start: start + batch_size]\n",
    "        yield data_to_matrices(data, good_last_indeces[ix], count_prev_good_indeces_limited, emb_size)\n",
    "\n",
    "\n",
    "def get_data_for_prediction(data_test, good_last_indeces, count_prev_good_indeces_limited, \n",
    "                            count_words, batch_size, shuffle=False, return_answers=True, \n",
    "                            min_count_prev_queries=1):\n",
    "    \n",
    "    prev_counts = count_prev_good_indeces_limited[good_last_indeces]\n",
    "    begin_indeces = good_last_indeces - prev_counts + 1\n",
    "        \n",
    "    all_prev_queries = []\n",
    "    all_next_queries = []\n",
    "    for i in range(len(begin_indeces)):\n",
    "        if good_last_indeces[i] - begin_indeces[i] <= min_count_prev_queries:\n",
    "            continue\n",
    "            \n",
    "        query_sequence_prev = [data_test[j][1] for j in range(begin_indeces[i], good_last_indeces[i])]\n",
    "        \n",
    "        query_next = data_test[good_last_indeces[i]][1]\n",
    "        \n",
    "        all_prev_queries.append(query_sequence_prev)\n",
    "        all_next_queries.append(query_next)\n",
    "        \n",
    "    all_prev_queries = np.array(all_prev_queries)\n",
    "    all_next_queries = np.array(all_next_queries)\n",
    "        \n",
    "    if shuffle:\n",
    "        indices = np.random.permutations(np.arange(len(good_last_indeces)))\n",
    "    else:\n",
    "        indices = np.arange(len(good_last_indeces))\n",
    "        \n",
    "    for start in range(0, len(indices), batch_size):\n",
    "        ix = indices[start: start + batch_size]\n",
    "        yield test_queries_to_matrices(all_prev_queries[ix], all_next_queries[ix], \n",
    "                                       count_words, return_answers=return_answers)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приступим к написанию модели из статьи https://arxiv.org/pdf/1507.02221.pdf. В отличие от модели из статьи я использую свой эмбеддинг слой, так как предобученных эмбеддингов для BPE токенов нет (в статье не написано какие эмбеддинги они использовали).  Модель из статьи состоит из трейх разных уровней : уровень запроса, уровень сессии и декодера. Для каждого из уровней напишем свой класс. <br>\n",
    "Заметим, что декодер должен уметь делать пословные предсказания и брать только что предсказаное слово на вход в следующий момент времени (для предсказания)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryLevelRNN(nn.Module):\n",
    "    def __init__(self, vocab_len=len(all_words), embedding_size=300, rnn_num_units=1000):\n",
    "        super(self.__class__,self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_len, embedding_dim=embedding_size)\n",
    "        self.num_units = rnn_num_units\n",
    "        self.rnn = nn.GRU(input_size=embedding_size, hidden_size=rnn_num_units)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.embeddings(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        init_h = torch.zeros(1, batch_size, self.num_units).to(device)\n",
    "        output_h, last_h = self.rnn(x, init_h)\n",
    "        return last_h[0]  ## chech if it returns (1, batch, hidden_size)\n",
    "    \n",
    "class SessionLevelRNN(nn.Module):\n",
    "    def __init__(self, embedding_size=1000, rnn_num_units=1500):\n",
    "        super(self.__class__,self).__init__()\n",
    "        self.num_units = rnn_num_units\n",
    "        self.rnn = nn.GRU(input_size=embedding_size, hidden_size=rnn_num_units)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.transpose(0, 1)\n",
    "        init_h = torch.zeros(1, batch_size, self.num_units).to(device)\n",
    "        output_h, last_h = self.rnn(x, init_h)\n",
    "        return output_h ## check if returns (seq_len, batch, hidden_size)\n",
    "    \n",
    "class DecoderCell(nn.Module):\n",
    "    def __init__(self, embedding_size=300, rnn_num_units=1000, init_state_size=1500):\n",
    "        super(self.__class__,self).__init__()\n",
    "        self.num_units = rnn_num_units\n",
    "        self.init_transform = nn.Linear(init_state_size, rnn_num_units)\n",
    "        self.gru_cell = nn.GRUCell(embedding_size, rnn_num_units)\n",
    "        self.transform_state = nn.Linear(rnn_num_units, embedding_size)\n",
    "        self.transform_word = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "        \n",
    "    def forward(self, x, prev_state, embedding_layer):\n",
    "        vocab_emb_matrix = embedding_layer.weight\n",
    "        next_h = self.gru_cell(x, prev_state)\n",
    "        w = self.transform_state(next_h) + self.transform_word(x)\n",
    "        logits = torch.matmul(vocab_emb_matrix, w.transpose(0, 1)).transpose(0, 1)\n",
    "        return next_h, logits\n",
    "    \n",
    "    def initial_state(self, session_state):\n",
    "        return nn.Tanh()(self.init_transform(session_state))\n",
    "    \n",
    "    \n",
    "def rnn_loop(rnn_cell, batch_ix, session_state, embedding_layer):\n",
    "    batch_size, max_length = batch_ix.size()\n",
    "    hid_state = rnn_cell.initial_state(session_state)\n",
    "    \n",
    "    batch_ix = batch_ix.transpose(0,1)\n",
    "    \n",
    "    #add ones(zeros) in the beggining\n",
    "    batch_ix = torch.cat([torch.ones((1, batch_size), dtype=torch.long).to(device) * bos_ix, batch_ix], dim=0)\n",
    "    \n",
    "    \n",
    "    all_logits = []\n",
    "\n",
    "    for x_t in batch_ix:\n",
    "        hid_state, logits_next = rnn_cell(embedding_layer(x_t), hid_state, embedding_layer)  # <-- here we call your one-step code\n",
    "        all_logits.append(logits_next)\n",
    "        \n",
    "    return torch.stack(all_logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим все уровни в одну модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_len=len(all_words), embedding_size=200, query_num_units=1000, \n",
    "                 session_num_units=1500, decoder_num_units=1000):\n",
    "        super(self.__class__,self).__init__()\n",
    "        \n",
    "        self.query_level = QueryLevelRNN(vocab_len, embedding_size, query_num_units).to(device)\n",
    "        self.session_level = SessionLevelRNN(query_num_units, session_num_units).to(device)\n",
    "        self.decoder = DecoderCell(embedding_size, decoder_num_units, session_num_units).to(device)\n",
    "        \n",
    "    def forward(self, queries):\n",
    "        queries_embeddings = []\n",
    "        for i in range(queries.shape[1]):\n",
    "            queries_embeddings.append(self.query_level(queries[:, i]))\n",
    "            \n",
    "        queries_embeddings = torch.stack(queries_embeddings, dim=1)\n",
    "        \n",
    "        session_states = self.session_level(queries_embeddings)\n",
    "        all_logits = []\n",
    "        for i in range(1, queries.shape[1]):\n",
    "            cur_session_embedding = session_states[i]\n",
    "            \n",
    "            ## give all words from next query without last word\n",
    "            logits = rnn_loop(self.decoder, queries[:,i], cur_session_embedding, self.query_level.embeddings)\n",
    "            all_logits.append(logits)\n",
    "            \n",
    "        return torch.stack(all_logits, dim=1)\n",
    "        \n",
    "            \n",
    "    def predict(self, previous_queries, current_words):\n",
    "        batch_size = previous_queries.shape[0]\n",
    "        \n",
    "        queries_embeddings = []\n",
    "        for i in range(previous_queries.shape[1]):\n",
    "            queries_embeddings.append(self.query_level(previous_queries[:, i])[:,None])\n",
    "            \n",
    "        queries_embeddings = torch.cat(queries_embeddings, dim=1)\n",
    "        \n",
    "        session_states = self.session_level(queries_embeddings)\n",
    "        session_last_state = session_states[-1]\n",
    "        \n",
    "        hid_state = self.decoder.initial_state(session_last_state)\n",
    "        \n",
    "        current_words = current_words.transpose(0,1)\n",
    "    \n",
    "        #add bos in the begining\n",
    "        current_words = torch.cat([torch.ones((1, batch_size), dtype=torch.long).to(device) * bos_ix, current_words], dim=0)\n",
    "        \n",
    "        all_logits = []\n",
    "\n",
    "        for x_t in current_words:\n",
    "            hid_state, logits_next = self.decoder(self.query_level.embeddings(x_t), hid_state, self.query_level.embeddings)\n",
    "        \n",
    "        all_logits.append(logits_next.cpu().detach().numpy())\n",
    "        next_word = logits_next.max(1)[1]\n",
    "        words = [next_word.cpu().numpy()]\n",
    "        \n",
    "        for i in range(max_word_count - current_words.shape[0]):\n",
    "            hid_state, logits_next = self.decoder(self.query_level.embeddings(next_word), hid_state, self.query_level.embeddings)\n",
    "            all_logits.append(logits_next.cpu().detach().numpy())\n",
    "            next_word = logits_next.max(1)[1]\n",
    "            words.append(next_word.cpu().numpy())\n",
    "        \n",
    "        return all_logits, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию подсчета лосса. Заметим, что важно не учитывать токен PAD при подсчете лосса, так как иначе мы научимся только почти всегда предсказывать токен PAD (в силу того, что его много)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakhrayv/.virtualenvs/lenas_py3.7/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=int(pad_ix), size_average = True)\n",
    "\n",
    "def compute_loss(model, queries, vocab_len=len(all_words), return_logits=False, **flags):\n",
    "    targets = torch.LongTensor(queries).to(device)\n",
    "    queries = torch.LongTensor(queries).to(device)\n",
    "    batchs_size, queries_count, time = queries.shape\n",
    "    \n",
    "    targets = torch.cat((targets, torch.ones(batchs_size, queries_count, 1, dtype=torch.long).to(device) * eos_ix), dim=2)\n",
    "    #delete first query (we won't predict it)\n",
    "    targets = targets[:,1:]\n",
    "    logits = model(queries)\n",
    "\n",
    "    if return_logits:\n",
    "        return criterion(logits.contiguous().view(-1, vocab_len), targets.contiguous().view(-1,)), logits\n",
    "#         return F.cross_entropy(logits.contiguous().view(-1, vocab_len), targets.contiguous().view(-1,)), logits\n",
    "    else:\n",
    "        return criterion(logits.contiguous().view(-1, vocab_len), targets.contiguous().view(-1,))\n",
    "#         return F.cross_entropy(logits.contiguous().view(-1, vocab_len), targets.contiguous().view(-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаемся. Последующие несколько ячеек - обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee2a74c664a46caad1947b78d0a7857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (10000 iteration) of 10 took 8444.271s\n",
      "  training loss (in-iteration): \t0.369836\n",
      "  validation loss (in-iteration): \t0.311349\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4132e7029394c148f38377e649fb579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 (20000 iteration) of 10 took 8451.308s\n",
      "  training loss (in-iteration): \t0.329902\n",
      "  validation loss (in-iteration): \t0.279879\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada6c18798ed4a649edd08debdfbf06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_generator = get_batch_generator(data_aol, good_last_indeces_for_batch, batch_size=64)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    for iteration in tqdm_notebook(range(steps_per_epoch)):\n",
    "        \n",
    "        batch_queries = next(batch_generator)\n",
    "        \n",
    "        loss, logits = compute_loss(model, batch_queries, return_logits=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        train_loss.append(loss.cpu().data.numpy())\n",
    "    \n",
    "\n",
    "    val_queries = get_val_batch_generator(data_aol_test, good_last_indeces_for_batch_test[:3000], \n",
    "                                          count_prev_good_indeces_limited_test,\n",
    "                                          batch_size=32, emb_size=300)\n",
    "    \n",
    "    count = 0\n",
    "    for batch_queries in val_queries:\n",
    "        loss, logits = compute_loss(model, batch_queries, return_logits=True)\n",
    "        val_loss.append(loss.cpu().data.numpy())\n",
    "        count += 1\n",
    "    \n",
    "    print(\"Epoch {} ({} iteration) of {} took {:.3f}s\".format(\n",
    "        epoch + 1, (epoch + 1) * steps_per_epoch, epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-steps_per_epoch :])))\n",
    "    print(\"  validation loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(val_loss[-count:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d7f019d1aa445fa5fe74836183b71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (10000 iteration) of 10 took 8429.673s\n",
      "  training loss (in-iteration): \t0.288248\n",
      "  validation loss (in-iteration): \t0.280234\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568d1ae2070d429a9335406afc5c5e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 (20000 iteration) of 10 took 8468.521s\n",
      "  training loss (in-iteration): \t0.273591\n",
      "  validation loss (in-iteration): \t0.256403\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de99d70ad5249419ae5226dc9e6ef82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 (30000 iteration) of 10 took 8442.152s\n",
      "  training loss (in-iteration): \t0.256440\n",
      "  validation loss (in-iteration): \t0.290331\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899c8b506b6f4b96b2a7555d0b395907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 (40000 iteration) of 10 took 8461.927s\n",
      "  training loss (in-iteration): \t0.249340\n",
      "  validation loss (in-iteration): \t0.204597\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d50261ef0d4241a213ec793c2722f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 (50000 iteration) of 10 took 8471.234s\n",
      "  training loss (in-iteration): \t0.240497\n",
      "  validation loss (in-iteration): \t0.221385\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456ebbf8159242c79eb9fba2a6e92cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 (60000 iteration) of 10 took 8459.908s\n",
      "  training loss (in-iteration): \t0.234936\n",
      "  validation loss (in-iteration): \t0.202633\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5a31947c08498c94279435f9b442a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_generator = get_batch_generator(data_aol, good_last_indeces_for_batch, batch_size=64)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    for iteration in tqdm_notebook(range(steps_per_epoch)):\n",
    "        \n",
    "        batch_queries = next(batch_generator)\n",
    "        \n",
    "        loss, logits = compute_loss(model, batch_queries, return_logits=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        train_loss.append(loss.cpu().data.numpy())\n",
    "    \n",
    "\n",
    "    val_queries = get_val_batch_generator(data_aol_test, good_last_indeces_for_batch_test[:3000], \n",
    "                                          count_prev_good_indeces_limited_test,\n",
    "                                          batch_size=32, emb_size=300)\n",
    "    \n",
    "    count = 0\n",
    "    for batch_queries in val_queries:\n",
    "        loss, logits = compute_loss(model, batch_queries, return_logits=True)\n",
    "        val_loss.append(loss.cpu().data.numpy())\n",
    "        count += 1\n",
    "    \n",
    "    print(\"Epoch {} ({} iteration) of {} took {:.3f}s\".format(\n",
    "        epoch + 1, (epoch + 1) * steps_per_epoch, epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-steps_per_epoch :])))\n",
    "    print(\"  validation loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(val_loss[-count:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11802d7d6e8a47fc80cacd90577e458e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 (10000 iteration) of 10 took 4607.510s\n",
      "  training loss (in-iteration): \t0.091005\n",
      "  validation loss (in-iteration): \t0.123855\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd3b9eb6e204a108a51939aa125dae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 (35000 iteration) of 10 took 4603.863s\n",
      "  training loss (in-iteration): \t0.089689\n",
      "  validation loss (in-iteration): \t0.137750\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2873d254d2944ee8967d2097d438caf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-250-932087ffbf9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mbatch_queries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_queries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-21b9664eed3e>\u001b[0m in \u001b[0;36mget_batch_generator\u001b[0;34m(data, good_last_indeces_for_batch, batch_size, emb_size)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mbatch_end_indeces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgood_last_indeces_for_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mdata_to_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_end_indeces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_prev_good_indeces_limited\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_generator = get_batch_generator(data_aol, good_last_indeces_for_batch, batch_size=128)\n",
    "epochs = 10\n",
    "steps_per_epoch = 5000\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    for iteration in tqdm_notebook(range(steps_per_epoch)):\n",
    "        \n",
    "        batch_queries = next(batch_generator)\n",
    "        \n",
    "        loss, logits = compute_loss(model, batch_queries, return_logits=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        train_loss.append(loss.cpu().data.numpy())\n",
    "    \n",
    "\n",
    "    val_queries = get_val_batch_generator(data_aol_test, good_last_indeces_for_batch_test[:100000], \n",
    "                                          count_prev_good_indeces_limited_test,\n",
    "                                          batch_size=32, emb_size=300)\n",
    "    \n",
    "    count = 0\n",
    "    for batch_queries in val_queries:\n",
    "        loss, logits = compute_loss(model, batch_queries, return_logits=True)\n",
    "        val_loss.append(loss.cpu().data.numpy())\n",
    "        count += 1\n",
    "    \n",
    "    print(\"Epoch {} ({} iteration) of {} took {:.3f}s\".format(\n",
    "        epoch + 1, (epoch + 1) * steps_per_epoch, epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-steps_per_epoch :])))\n",
    "    print(\"  validation loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(val_loss[-count:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 (50000 iteration) of 10 took 846.671s\n",
      "  training loss (in-iteration): \t0.088184\n",
      "  validation loss (in-iteration): \t0.130126\n"
     ]
    }
   ],
   "source": [
    "val_queries = get_val_batch_generator(data_aol_test, good_last_indeces_for_batch_test[:100000], \n",
    "                                      count_prev_good_indeces_limited_test,\n",
    "                                      batch_size=32, emb_size=300) \n",
    "count = 0\n",
    "for batch_queries in val_queries:\n",
    "    loss, logits = compute_loss(model, batch_queries, return_logits=True)\n",
    "    val_loss.append(loss.cpu().data.numpy())\n",
    "    count += 1\n",
    "\n",
    "print(\"Epoch {} ({} iteration) of {} took {:.3f}s\".format(\n",
    "    epoch + 1, (epoch + 1) * steps_per_epoch, epochs, time.time() - start_time))\n",
    "print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "    np.mean(train_loss[-steps_per_epoch :])))\n",
    "print(\"  validation loss (in-iteration): \\t{:.6f}\".format(\n",
    "    np.mean(val_loss[-count:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'trained_model_3.pth')\n",
    "\n",
    "# model = torch.load(PATH)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 (50000 iteration) of 10 took 27066.297s\n",
      "  training loss (in-iteration): \t0.100864\n",
      "  validation loss (in-iteration): \t0.127759\n"
     ]
    }
   ],
   "source": [
    "val_queries = get_val_batch_generator(data_aol_test, good_last_indeces_for_batch_test[:100000], \n",
    "                                      count_prev_good_indeces_limited_test,\n",
    "                                      batch_size=32, emb_size=300) \n",
    "count = 0\n",
    "for batch_queries in val_queries:\n",
    "    loss, logits = compute_loss(model, batch_queries, return_logits=True)\n",
    "    val_loss.append(loss.cpu().data.numpy())\n",
    "    count += 1\n",
    "\n",
    "print(\"Epoch {} ({} iteration) of {} took {:.3f}s\".format(\n",
    "    epoch + 1, (epoch + 1) * steps_per_epoch, epochs, time.time() - start_time))\n",
    "print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "    np.mean(train_loss[-steps_per_epoch :])))\n",
    "print(\"  validation loss (in-iteration): \\t{:.6f}\".format(\n",
    "    np.mean(val_loss[-count:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "Добавим self-attention на уровне сессии. То есть состояние, которое получает на вход RNN-ка уровня сессии будет являться не состоянием, которое вернула RNN на предыдущем шаге, а attention-механизм от всех предыдущих состояний RNN уровня сессии и текущего запроса. <br>\n",
    "В качестве attention скора я выбрала concat. Пусть $s_j$ - выходное состояние RNN уровня сессии на этапе j (имеет информацию о первых j запросах), а $q_t$ - эмбеддинг запроса под номером t ($t > j$), полученный при помощи первого уровня - уровня запроса .Тогда $score(s_j, q_t) = v^T tanh(W[s_j;q_t])$, что и называется concat скором.\n",
    "После этого вычисляются коэффициенты $\\alpha = softmax((score(s_1, q_t), ..., score(s_{t-1}, q_t)))$ и новое состояние RNN уровня сессии, которое подается на вход RNN на следующем этапе вычисляется так : $$new\\_s_t = \\sum_{j=0}^{t - 1} \\alpha_j s_j$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionLevelRNNCell(nn.Module):\n",
    "    def __init__(self, embedding_size=1000, rnn_num_units=1500, attention_size=500):\n",
    "        super(self.__class__,self).__init__()\n",
    "        self.num_units = rnn_num_units\n",
    "        self.rnn_cell = nn.GRUCell(input_size=embedding_size, hidden_size=rnn_num_units)\n",
    "        self.linear_attention = nn.Linear(rnn_num_units + embedding_size, attention_size)\n",
    "        self.v_attention = nn.Linear(attention_size, 1, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        next_h = self.rnn_cell(x, prev_state)\n",
    "        return next_h\n",
    "       \n",
    "    def initial_state(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.num_units).to(device)\n",
    "\n",
    "\n",
    "def attention_function(q_t, s_j, linear_attention, v_attention):\n",
    "    dot = nn.Tanh()(linear_attention(torch.cat([q_t, s_j], dim=1)))\n",
    "    \n",
    "    return v_attention(dot)\n",
    "\n",
    "def rnn_loop_session(rnn_cell, batch_ix):\n",
    "    batch_size, max_length = batch_ix.size()[:2]\n",
    "    hid_state = rnn_cell.initial_state(batch_size)\n",
    "    batch_ix = batch_ix.transpose(0,1)\n",
    "    \n",
    "    hid_state = rnn_cell(batch_ix[0], hid_state)\n",
    "    hidden_states = [hid_state]\n",
    "    \n",
    "    \n",
    "    for t, q_t in enumerate(batch_ix[1:]):\n",
    "        # compute alphas\n",
    "        alphas = []\n",
    "        for j in range(t + 1):\n",
    "            alphas.append(attention_function(q_t, hidden_states[j], \n",
    "                                             rnn_cell.linear_attention, \n",
    "                                             rnn_cell.v_attention))\n",
    "            \n",
    "        # normalize alphas to sum to 1\n",
    "        alphas = nn.Softmax()(torch.cat(alphas, dim=1))\n",
    "        \n",
    "        #compute new state\n",
    "        new_hidden_state = torch.zeros(batch_size, rnn_cell.num_units).to(device)\n",
    "        \n",
    "        for j in range(t):\n",
    "            new_hidden_state += alphas[:,t][:,None] * hidden_states[j]\n",
    "        \n",
    "        hid_state = new_hidden_state\n",
    "        \n",
    "        hid_state = rnn_cell(q_t, hid_state)\n",
    "        hidden_states.append(hid_state)\n",
    "    \n",
    "    return torch.stack(hidden_states, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нода beam-search-а. Что это и зачем будет рассказано позже. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamNode(object):\n",
    "    def __init__(self, parent_node, cur_state, pred_word, cost):\n",
    "        super(BeamNode, self).__init__()\n",
    "        self.pred_word = pred_word\n",
    "        self.parent_node = parent_node # parent Node, None for root\n",
    "        self.cur_state = cur_state # recurrent layer hidden state\n",
    "        self.cum_cost = parent_node.cum_cost + cost if parent_node else cost # e.g. -log(p) of sequence up to current node (including)\n",
    "        self.length = 1 if parent_node is None else parent_node.length + 1\n",
    "        self._sequence = None\n",
    "    \n",
    "    def to_sequence(self):\n",
    "        # Return sequence of nodes from root to current node.\n",
    "        if not self._sequence:\n",
    "            self._sequence = []\n",
    "            current_node = self\n",
    "            while current_node:\n",
    "                self._sequence.append(current_node)\n",
    "                current_node = current_node.parent_node\n",
    "            self._sequence = self._sequence[::-1]\n",
    "        return self._sequence\n",
    "\n",
    "    def to_sequence_of_values(self):\n",
    "        return [int(s.pred_word.cpu().numpy()) for s in self.to_sequence()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelAttention(nn.Module):\n",
    "    def __init__(self, vocab_len=len(all_words), embedding_size=200, query_num_units=1000, \n",
    "                 session_num_units=1500, decoder_num_units=1000, attention_size=500):\n",
    "        super(self.__class__,self).__init__()\n",
    "        \n",
    "        self.query_level = QueryLevelRNN(vocab_len, embedding_size, query_num_units).to(device)\n",
    "        self.session_level = SessionLevelRNNCell(query_num_units, session_num_units, attention_size).to(device)\n",
    "        self.decoder = DecoderCell(embedding_size, decoder_num_units, session_num_units).to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self, queries):\n",
    "        queries_embeddings = []\n",
    "        for i in range(queries.shape[1]):\n",
    "            queries_embeddings.append(self.query_level(queries[:, i]))\n",
    "            \n",
    "        queries_embeddings = torch.stack(queries_embeddings, dim=1)\n",
    "        \n",
    "        session_states = rnn_loop_session(self.session_level, queries_embeddings)\n",
    "\n",
    "        all_logits = []\n",
    "        for i in range(1, queries.shape[1]):\n",
    "            cur_session_embedding = session_states[i]\n",
    "            ## give all words from next query without last word\n",
    "            logits = rnn_loop(self.decoder, queries[:,i], cur_session_embedding, self.query_level.embeddings)\n",
    "            all_logits.append(logits)\n",
    "            \n",
    "        return torch.stack(all_logits, dim=1)\n",
    "        \n",
    "            \n",
    "    def predict(self, previous_queries, current_words):\n",
    "        batch_size = previous_queries.shape[0]\n",
    "        \n",
    "        queries_embeddings = []\n",
    "        for i in range(previous_queries.shape[1]):\n",
    "            queries_embeddings.append(self.query_level(previous_queries[:, i])[:,None])\n",
    "            \n",
    "        queries_embeddings = torch.cat(queries_embeddings, dim=1)\n",
    "        \n",
    "        session_states = rnn_loop_session(self.session_level, queries_embeddings)\n",
    "\n",
    "        session_last_state = session_states[-1]\n",
    "        \n",
    "        hid_state = self.decoder.initial_state(session_last_state)\n",
    "        \n",
    "        current_words = current_words.transpose(0,1)\n",
    "    \n",
    "        #add bos in the begining\n",
    "        current_words = torch.cat([torch.ones((1, batch_size), dtype=torch.long).to(device) * bos_ix, current_words], dim=0)\n",
    "        \n",
    "        all_logits = []\n",
    "\n",
    "        for x_t in current_words:\n",
    "            hid_state, logits_next = self.decoder(self.query_level.embeddings(x_t), hid_state, self.query_level.embeddings)\n",
    "        \n",
    "        all_logits.append(logits_next.cpu().detach().numpy())\n",
    "        next_word = logits_next.max(1)[1]\n",
    "        words = [next_word.cpu().numpy()]\n",
    "        \n",
    "        for i in range(max_word_count - current_words.shape[0]):\n",
    "            hid_state, logits_next = self.decoder(self.query_level.embeddings(next_word), hid_state, self.query_level.embeddings)\n",
    "            all_logits.append(logits_next.cpu().detach().numpy())\n",
    "            next_word = logits_next.max(1)[1]\n",
    "            words.append(next_word.cpu().numpy())\n",
    "        \n",
    "        return all_logits, words\n",
    "    \n",
    "    \n",
    "    def predict_beam_search(self, previous_queries, current_words, beam_width=4, num_hypotheses=1):\n",
    "        batch_size = previous_queries.shape[0]\n",
    "        count_prev_words = current_words.shape[1]\n",
    "\n",
    "        queries_embeddings = []\n",
    "        for i in range(previous_queries.shape[1]):\n",
    "            queries_embeddings.append(self.query_level(previous_queries[:, i])[:,None])\n",
    "\n",
    "        queries_embeddings = torch.cat(queries_embeddings, dim=1)\n",
    "\n",
    "        session_states = rnn_loop_session(self.session_level, queries_embeddings)\n",
    "        session_last_state = session_states[-1]\n",
    "\n",
    "        hid_state = self.decoder.initial_state(session_last_state)\n",
    "\n",
    "        current_words = current_words.transpose(0,1)\n",
    "\n",
    "        #add bos in the begining\n",
    "        current_words = torch.cat([torch.ones((1, batch_size), dtype=torch.long).to(device) * bos_ix, current_words], dim=0)\n",
    "\n",
    "        all_logits = []\n",
    "        for x_t in current_words[:-1]:\n",
    "            hid_state, logits_next = self.decoder(self.query_level.embeddings(x_t), hid_state, \n",
    "                                                  self.query_level.embeddings)\n",
    "\n",
    "\n",
    "        hypotheses_words = []\n",
    "        hypotheses = []\n",
    "        for i in range(batch_size):\n",
    "            words, nodes = self.beam_search(hid_state[i], current_words[-1,i],\n",
    "                                            beam_width=beam_width, num_hypotheses=num_hypotheses, \n",
    "                                            max_length=(max_word_count - count_prev_words))\n",
    "            \n",
    "            hypotheses_words.append(words)\n",
    "            hypotheses.append(nodes)\n",
    "\n",
    "        return hypotheses_words, hypotheses\n",
    "    \n",
    "    def beam_search(self, init_state, init_word, beam_width=4, num_hypotheses=1, max_length=9):\n",
    "        next_fringe = [BeamNode(parent_node=None, cur_state=init_state, pred_word=init_word, cost=0.0)]\n",
    "        hypotheses = []\n",
    "\n",
    "        for _ in range(max_length):\n",
    "\n",
    "            fringe = []\n",
    "            for node in next_fringe:\n",
    "                if node.pred_word == eos_ix:\n",
    "                    hypotheses.append(node)\n",
    "                else:\n",
    "                    fringe.append(node)\n",
    "\n",
    "            if not fringe:\n",
    "                break\n",
    "                \n",
    "            preds_words = torch.cat([node.pred_word[None] for node in fringe], dim=0)\n",
    "\n",
    "            hid_states = torch.stack([node.cur_state for node in fringe], dim=0)\n",
    "\n",
    "            hid_state, logits_next = self.decoder(self.query_level.embeddings(preds_words), hid_states, \n",
    "                                                  self.query_level.embeddings)\n",
    "\n",
    "            log_probs = nn.LogSoftmax()(logits_next)\n",
    "            log_probs_next, words_next = log_probs.topk(beam_width, dim=1, sorted=False)\n",
    "\n",
    "            next_fringe = []\n",
    "\n",
    "            for log_probs_next_n, words_next_n, hid_state_n, node_prev in zip(log_probs_next, words_next, \n",
    "                                                                              hid_state, fringe):\n",
    "\n",
    "                for word, log_prob in zip(words_next_n, log_probs_next_n):\n",
    "                    new_node = BeamNode(parent_node=node_prev, cur_state=hid_state_n, \n",
    "                                        pred_word=word, cost=-log_prob)\n",
    "                    if new_node.length == max_length + 1:\n",
    "                        hypotheses.append(new_node)\n",
    "                    next_fringe.append(new_node)\n",
    "\n",
    "\n",
    "            next_fringe = sorted(next_fringe, key=lambda node: node.cum_cost)[:beam_width] # may move this into loop to save memory\n",
    "\n",
    "        hypotheses.sort(key=lambda node: node.cum_cost)\n",
    "\n",
    "        return [end_node.to_sequence_of_values() for end_node in hypotheses[:num_hypotheses]], [end_node.to_sequence() for end_node in hypotheses[:num_hypotheses]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelAttention().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19acffc2a17446288f85ceeb8a5b81b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakhrayv/.virtualenvs/lenas_py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 (30000 iteration) of 10 took 4557.624s\n",
      "  training loss (in-iteration): \t0.079599\n",
      "  validation loss (in-iteration): \t0.074495\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c158a32a91d64fc1bcb9509ffc9ce4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 (40000 iteration) of 10 took 4567.522s\n",
      "  training loss (in-iteration): \t0.067072\n",
      "  validation loss (in-iteration): \t0.062544\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5646decdcd8241fc82f9aeda8c5bd8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_generator = get_batch_generator(data_aol, good_last_indeces_for_batch, batch_size=128)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    for iteration in tqdm_notebook(range(steps_per_epoch)):\n",
    "        \n",
    "        batch_queries = next(batch_generator)\n",
    "        if not len(batch_queries):\n",
    "            continue\n",
    "            \n",
    "        loss = compute_loss(model, batch_queries, return_logits=False)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        train_loss.append(loss.cpu().data.numpy())\n",
    "    \n",
    "\n",
    "    val_queries = get_val_batch_generator(data_aol_test, good_last_indeces_for_batch_test[:100000], \n",
    "                                          count_prev_good_indeces_limited_test,\n",
    "                                          batch_size=64, emb_size=300)\n",
    "    \n",
    "    count = 0\n",
    "    for batch_queries in val_queries:\n",
    "        if not len(batch_queries):\n",
    "            continue\n",
    "        loss = compute_loss(model, batch_queries, return_logits=False)\n",
    "        val_loss.append(loss.cpu().data.numpy())\n",
    "        count += 1\n",
    "    \n",
    "    print(\"Epoch {} ({} iteration) of {} took {:.3f}s\".format(\n",
    "        epoch + 1, (epoch + 1) * steps_per_epoch, epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-steps_per_epoch :])))\n",
    "    print(\"  validation loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(val_loss[-count:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c791a9b7df34913bde2be50c44ba6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakhrayv/.virtualenvs/lenas_py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 (5000 iteration) of 10 took 4563.373s\n",
      "  training loss (in-iteration): \t0.055275\n",
      "  validation loss (in-iteration): \t0.058100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edafb7eddd443678f17bb9b603f5002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 (10000 iteration) of 10 took 4566.526s\n",
      "  training loss (in-iteration): \t0.053705\n",
      "  validation loss (in-iteration): \t0.050826\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a31ee45a274417b8753de4a171323c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 (15000 iteration) of 10 took 4558.100s\n",
      "  training loss (in-iteration): \t0.050076\n",
      "  validation loss (in-iteration): \t0.046923\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2437d5815cf4902958ad0b53fc6a2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 (20000 iteration) of 10 took 4567.519s\n",
      "  training loss (in-iteration): \t0.048213\n",
      "  validation loss (in-iteration): \t0.045220\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fca4f9d9db748b2894bf5f26710f193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 (25000 iteration) of 10 took 4563.240s\n",
      "  training loss (in-iteration): \t0.045775\n",
      "  validation loss (in-iteration): \t0.049193\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690e606f4a7c49cc97ac15f1e98354c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 (30000 iteration) of 10 took 4561.560s\n",
      "  training loss (in-iteration): \t0.044302\n",
      "  validation loss (in-iteration): \t0.044828\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a80146820f14c6f94ff52537d258194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 (35000 iteration) of 10 took 4565.101s\n",
      "  training loss (in-iteration): \t0.043300\n",
      "  validation loss (in-iteration): \t0.042068\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d6551c53d74be4ba29ae5f3af820a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_generator = get_batch_generator(data_aol, good_last_indeces_for_batch, batch_size=128)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    for iteration in tqdm_notebook(range(steps_per_epoch)):\n",
    "        \n",
    "        batch_queries = next(batch_generator)\n",
    "        if not len(batch_queries):\n",
    "            continue\n",
    "            \n",
    "        loss = compute_loss(model, batch_queries, return_logits=False)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        train_loss.append(loss.cpu().data.numpy())\n",
    "    \n",
    "\n",
    "    val_queries = get_val_batch_generator(data_aol_test, good_last_indeces_for_batch_test[:100000], \n",
    "                                          count_prev_good_indeces_limited_test,\n",
    "                                          batch_size=64, emb_size=300)\n",
    "    \n",
    "    count = 0\n",
    "    for batch_queries in val_queries:\n",
    "        if not len(batch_queries):\n",
    "            continue\n",
    "        loss = compute_loss(model, batch_queries, return_logits=False)\n",
    "        val_loss.append(loss.cpu().data.numpy())\n",
    "        count += 1\n",
    "    \n",
    "    print(\"Epoch {} ({} iteration) of {} took {:.3f}s\".format(\n",
    "        epoch + 1, (epoch + 1) * steps_per_epoch, epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-steps_per_epoch :])))\n",
    "    print(\"  validation loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(val_loss[-count:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26d10fbdc5c43e682faafc6517fb1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakhrayv/.virtualenvs/lenas_py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 (5000 iteration) of 10 took 4698.014s\n",
      "  training loss (in-iteration): \t0.026656\n",
      "  validation loss (in-iteration): \t0.030010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db05c287192749ae841400ccdc2cdbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 (10000 iteration) of 10 took 4701.227s\n",
      "  training loss (in-iteration): \t0.026951\n",
      "  validation loss (in-iteration): \t0.029880\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af8c459df2c489db0d1aa5f9962eb64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 (15000 iteration) of 10 took 4703.050s\n",
      "  training loss (in-iteration): \t0.026247\n",
      "  validation loss (in-iteration): \t0.028640\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac5945cc28b4f138822a6385dc01e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 (25000 iteration) of 10 took 4699.495s\n",
      "  training loss (in-iteration): \t0.024044\n",
      "  validation loss (in-iteration): \t0.028068\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521c6940a2bb4213a6572afc621e7f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 (50000 iteration) of 10 took 4691.346s\n",
      "  training loss (in-iteration): \t0.020712\n",
      "  validation loss (in-iteration): \t0.031169\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = 5000\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "\n",
    "batch_generator = get_batch_generator(data_aol, good_last_indeces_for_batch, batch_size=256, \n",
    "                                      min_count_prev_queries=2, max_batch_size=128)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    for iteration in tqdm_notebook(range(steps_per_epoch)):\n",
    "        \n",
    "        batch_queries = next(batch_generator)\n",
    "        if not len(batch_queries):\n",
    "            continue\n",
    "            \n",
    "        loss = compute_loss(model, batch_queries, return_logits=False)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        train_loss.append(loss.cpu().data.numpy())\n",
    "    \n",
    "\n",
    "    val_queries = get_val_batch_generator(data_aol_test, good_last_indeces_for_batch_test[:100000], \n",
    "                                          count_prev_good_indeces_limited_test,\n",
    "                                          batch_size=64, emb_size=300)\n",
    "    \n",
    "    count = 0\n",
    "    for batch_queries in val_queries:\n",
    "        if not len(batch_queries):\n",
    "            continue\n",
    "        loss = compute_loss(model, batch_queries, return_logits=False)\n",
    "        val_loss.append(loss.cpu().data.numpy())\n",
    "        count += 1\n",
    "    \n",
    "    print(\"Epoch {} ({} iteration) of {} took {:.3f}s\".format(\n",
    "        epoch + 1, (epoch + 1) * steps_per_epoch, epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-steps_per_epoch :])))\n",
    "    print(\"  validation loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(val_loss[-count:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdae155daff4f8c842c3606e6e26b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakhrayv/.virtualenvs/lenas_py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 (5000 iteration) of 1 took 4725.586s\n",
      "  training loss (in-iteration): \t0.019545\n",
      "  validation loss (in-iteration): \t0.025158\n"
     ]
    }
   ],
   "source": [
    "batch_generator = get_batch_generator(data_aol, good_last_indeces_for_batch, batch_size=256, \n",
    "                                      min_count_prev_queries=2, max_batch_size=136)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    for iteration in tqdm_notebook(range(steps_per_epoch)):\n",
    "        \n",
    "        batch_queries = next(batch_generator)\n",
    "        if not len(batch_queries):\n",
    "            continue\n",
    "            \n",
    "        loss = compute_loss(model, batch_queries, return_logits=False)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        train_loss.append(loss.cpu().data.numpy())\n",
    "    \n",
    "\n",
    "    val_queries = get_val_batch_generator(data_aol_test, good_last_indeces_for_batch_test[:100000], \n",
    "                                          count_prev_good_indeces_limited_test,\n",
    "                                          batch_size=64, emb_size=300)\n",
    "    \n",
    "    count = 0\n",
    "    for batch_queries in val_queries:\n",
    "        if not len(batch_queries):\n",
    "            continue\n",
    "        loss = compute_loss(model, batch_queries, return_logits=False)\n",
    "        val_loss.append(loss.cpu().data.numpy())\n",
    "        count += 1\n",
    "    \n",
    "    print(\"Epoch {} ({} iteration) of {} took {:.3f}s\".format(\n",
    "        epoch + 1, (epoch + 1) * steps_per_epoch, epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-steps_per_epoch :])))\n",
    "    print(\"  validation loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(val_loss[-count:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Лосс на валидации стал больше, чем в 5 раза меньше, чем был!!** Сохраняем модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakhrayv/.virtualenvs/lenas_py3.7/lib/python3.7/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type ModelAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/shakhrayv/.virtualenvs/lenas_py3.7/lib/python3.7/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type QueryLevelRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/shakhrayv/.virtualenvs/lenas_py3.7/lib/python3.7/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type SessionLevelRNNCell. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/shakhrayv/.virtualenvs/lenas_py3.7/lib/python3.7/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type DecoderCell. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'trained_model_self_attention_0.025158.pth')\n",
    "torch.save(model.state_dict(), 'trained_model_self_attention_0.025158_dict.pth')\n",
    "# model = torch.load('trained_model_two_attentions_2.pth')\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention in decoder\n",
    "Attention over queries embeddings\n",
    "Еще больше внимания!\n",
    "Кроме self-attention-а, добавленного ранее, добаивим еще attention в декодере. Этот attention будет при генерации очередного слова запроса смотреть на эмбеддинги предыдущих запросов. Таким образом, это поможет обращать больше внимания на все предыдущие запросы (а не отдавать большое предпочтение одному предыдущему, так как про него мы помним лучше всего). Здесь в качестве attention-скора я выбрала general score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_attention_function(w, q_j, linear_attention):\n",
    "    return torch.sum(linear_attention(w) * q_j, dim=1)\n",
    "    \n",
    "\n",
    "class DecoderCellAttention(nn.Module):\n",
    "    def __init__(self, embedding_size=300, rnn_num_units=1000, init_state_size=1500, query_num_units=1000):\n",
    "        super(self.__class__,self).__init__()\n",
    "        self.num_units = rnn_num_units\n",
    "        self.query_num_units = query_num_units\n",
    "        self.init_transform = nn.Linear(init_state_size, rnn_num_units)\n",
    "        self.gru_cell = nn.GRUCell(embedding_size + embedding_size, rnn_num_units)\n",
    "        self.transform_state = nn.Linear(rnn_num_units, embedding_size)\n",
    "        self.transform_word = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "        self.linear_attention = nn.Linear(embedding_size, query_num_units)\n",
    "        \n",
    "        self.linear_query_transform = nn.Linear(query_num_units, embedding_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, prev_state, embedding_layer, prev_queries_embeddings):\n",
    "        \n",
    "        batch_size, count_prev_queries = prev_queries_embeddings.shape[:2]\n",
    "        prev_queries_embeddings = prev_queries_embeddings.transpose(0, 1)\n",
    "        \n",
    "        #compute alphas\n",
    "        alphas = []\n",
    "        \n",
    "        for q_j in prev_queries_embeddings:\n",
    "            alphas.append(decoder_attention_function(x, q_j, self.linear_attention))\n",
    "            \n",
    "        alphas = nn.Softmax()(torch.stack(alphas, dim=1))\n",
    "        \n",
    "        # compute attention addition\n",
    "        addition_queries = torch.zeros(batch_size, self.query_num_units).to(device)\n",
    "        for j in range(count_prev_queries):\n",
    "            addition_queries += alphas[:,j][:,None] * prev_queries_embeddings[j]\n",
    "            \n",
    "        # concat addition info to word\n",
    "        new_x = torch.cat([x, self.linear_query_transform(addition_queries)], dim=1)\n",
    "       \n",
    "        # compute logits\n",
    "        vocab_emb_matrix = embedding_layer.weight\n",
    "        next_h = self.gru_cell(new_x, prev_state)\n",
    "        w = self.transform_state(next_h) + self.transform_word(x)\n",
    "        logits = torch.matmul(vocab_emb_matrix, w.transpose(0, 1)).transpose(0, 1)\n",
    "        return next_h, logits\n",
    "    \n",
    "    def initial_state(self, session_state):\n",
    "        return nn.Tanh()(self.init_transform(session_state))\n",
    "    \n",
    "    \n",
    "    \n",
    "def rnn_loop_decoder(rnn_cell, batch_ix, session_state, embedding_layer, prev_queries_embeddings):\n",
    "    \"\"\"\n",
    "    Computes log P(next_character) for all time-steps in lines_ix\n",
    "    :param lines_ix: an int32 matrix of shape [batch, time], output of to_matrix(lines)\n",
    "    \"\"\"\n",
    "    batch_size, max_length = batch_ix.size()\n",
    "    hid_state = rnn_cell.initial_state(session_state)\n",
    "    \n",
    "    batch_ix = batch_ix.transpose(0,1)\n",
    "    \n",
    "    #add ones(zeros) in the beggining\n",
    "    batch_ix = torch.cat([torch.ones((1, batch_size), dtype=torch.long).to(device) * bos_ix, batch_ix], dim=0)\n",
    "    \n",
    "    \n",
    "    all_logits = []\n",
    "\n",
    "    for x_t in batch_ix:\n",
    "        hid_state, logits_next = rnn_cell(embedding_layer(x_t), hid_state, \n",
    "                                          embedding_layer, prev_queries_embeddings)\n",
    "        all_logits.append(logits_next)\n",
    "        \n",
    "    return torch.stack(all_logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTwoAttention(nn.Module):\n",
    "    def __init__(self, vocab_len=len(all_words), embedding_size=200, query_num_units=1000, \n",
    "                 session_num_units=1500, decoder_num_units=1000, attention_size=500):\n",
    "        super(self.__class__,self).__init__()\n",
    "        \n",
    "        self.query_level = QueryLevelRNN(vocab_len, embedding_size, query_num_units).to(device)\n",
    "        self.session_level = SessionLevelRNNCell(query_num_units, session_num_units, attention_size).to(device)\n",
    "        self.decoder = DecoderCellAttention(embedding_size, decoder_num_units, session_num_units).to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self, queries):\n",
    "        queries_embeddings = []\n",
    "        for i in range(queries.shape[1]):\n",
    "            queries_embeddings.append(self.query_level(queries[:, i]))\n",
    "            \n",
    "        queries_embeddings = torch.stack(queries_embeddings, dim=1)\n",
    "        \n",
    "        session_states = rnn_loop_session(self.session_level, queries_embeddings)\n",
    "\n",
    "        all_logits = []\n",
    "        for i in range(1, queries.shape[1]):\n",
    "            cur_session_embedding = session_states[i]\n",
    "            ## give all words from next query without last word\n",
    "            logits = rnn_loop_decoder(self.decoder, queries[:,i], cur_session_embedding, \n",
    "                                      self.query_level.embeddings, queries_embeddings[:,:i])\n",
    "            all_logits.append(logits)\n",
    "            \n",
    "        return torch.stack(all_logits, dim=1)\n",
    "        \n",
    "            \n",
    "    def predict(self, previous_queries, current_words):\n",
    "        batch_size = previous_queries.shape[0]\n",
    "        \n",
    "        queries_embeddings = []\n",
    "        for i in range(previous_queries.shape[1]):\n",
    "            queries_embeddings.append(self.query_level(previous_queries[:, i])[:,None])\n",
    "            \n",
    "        queries_embeddings = torch.cat(queries_embeddings, dim=1)\n",
    "        \n",
    "#         session_states = self.session_level(queries_embeddings)\n",
    "        session_states = rnn_loop_session(self.session_level, queries_embeddings)\n",
    "        session_last_state = session_states[-1]\n",
    "        \n",
    "        hid_state = self.decoder.initial_state(session_last_state)\n",
    "        \n",
    "        current_words = current_words.transpose(0,1)\n",
    "    \n",
    "        #add bos in the begining\n",
    "        current_words = torch.cat([torch.ones((1, batch_size), dtype=torch.long).to(device) * bos_ix, current_words], dim=0)\n",
    "        \n",
    "        all_logits = []\n",
    "        for x_t in current_words:\n",
    "            hid_state, logits_next = self.decoder(self.query_level.embeddings(x_t), hid_state, \n",
    "                                                  self.query_level.embeddings, queries_embeddings)\n",
    "        \n",
    "        all_logits.append(logits_next.cpu().detach().numpy())\n",
    "        next_word = logits_next.max(1)[1]\n",
    "        words = [next_word.cpu().numpy()]\n",
    "        for i in range(max_word_count - current_words.shape[0]):\n",
    "            hid_state, logits_next = self.decoder(self.query_level.embeddings(next_word), hid_state, \n",
    "                                                  self.query_level.embeddings, queries_embeddings)\n",
    "            \n",
    "            all_logits.append(logits_next.cpu().detach().numpy())\n",
    "            next_word = logits_next.max(1)[1]\n",
    "            words.append(next_word.cpu().numpy())\n",
    "        \n",
    "        return all_logits, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelTwoAttention().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаемся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53fbedc92ce48e9a84389e21fbe59ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakhrayv/.virtualenvs/lenas_py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/shakhrayv/.virtualenvs/lenas_py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 (5000 iteration) of 10 took 4771.819s\n",
      "  training loss (in-iteration): \t0.887201\n",
      "  validation loss (in-iteration): \t0.253390\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95c9f248866408682b00a25ac3f1c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 (10000 iteration) of 10 took 4770.929s\n",
      "  training loss (in-iteration): \t0.222591\n",
      "  validation loss (in-iteration): \t0.188359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24aa27b86455454d88fc54f425dd576e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 (15000 iteration) of 10 took 4774.126s\n",
      "  training loss (in-iteration): \t0.159412\n",
      "  validation loss (in-iteration): \t0.123379\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea0993b72e1453e906e7cbdd38f083c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 (20000 iteration) of 10 took 4771.100s\n",
      "  training loss (in-iteration): \t0.129754\n",
      "  validation loss (in-iteration): \t0.105433\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81db110eb4ba4f66aa7289332830665c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 (25000 iteration) of 10 took 4774.012s\n",
      "  training loss (in-iteration): \t0.108788\n",
      "  validation loss (in-iteration): \t0.092166\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17cbc6263e8402292fe1e9cd3974f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 (30000 iteration) of 10 took 4778.019s\n",
      "  training loss (in-iteration): \t0.096358\n",
      "  validation loss (in-iteration): \t0.097755\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a279f703894f0db878f63898c0c874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = 5000\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "\n",
    "batch_generator = get_batch_generator(data_aol, good_last_indeces_for_batch, batch_size=128)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    for iteration in tqdm_notebook(range(steps_per_epoch)):\n",
    "        \n",
    "        batch_queries = next(batch_generator)\n",
    "        if not len(batch_queries):\n",
    "            continue\n",
    "            \n",
    "        loss = compute_loss(model, batch_queries, return_logits=False)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        train_loss.append(loss.cpu().data.numpy())\n",
    "    \n",
    "\n",
    "    val_queries = get_val_batch_generator(data_aol_test, good_last_indeces_for_batch_test[:100000], \n",
    "                                          count_prev_good_indeces_limited_test,\n",
    "                                          batch_size=64, emb_size=300)\n",
    "    \n",
    "    count = 0\n",
    "    for batch_queries in val_queries:\n",
    "        if not len(batch_queries):\n",
    "            continue\n",
    "        loss = compute_loss(model, batch_queries, return_logits=False)\n",
    "        val_loss.append(loss.cpu().data.numpy())\n",
    "        count += 1\n",
    "    \n",
    "    print(\"Epoch {} ({} iteration) of {} took {:.3f}s\".format(\n",
    "        epoch + 1, (epoch + 1) * steps_per_epoch, epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-steps_per_epoch :])))\n",
    "    print(\"  validation loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(val_loss[-count:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 (40000 iteration) of 10 took 30688.004s\n",
      "  training loss (in-iteration): \t0.083708\n",
      "  validation loss (in-iteration): \t0.081585\n"
     ]
    }
   ],
   "source": [
    "print(\"Epoch {} ({} iteration) of {} took {:.3f}s\".format(\n",
    "    epoch + 1, (epoch + 1) * steps_per_epoch, epochs, time.time() - start_time))\n",
    "print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "    np.mean(train_loss[-steps_per_epoch :])))\n",
    "print(\"  validation loss (in-iteration): \\t{:.6f}\".format(\n",
    "    np.mean(val_loss[-count:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'trained_model_two_attentions_3.pth')\n",
    "\n",
    "# model = torch.load('trained_model_two_attentions_2.pth')\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3bfbbacd7f4232b399d3e9ae9c154f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakhrayv/.virtualenvs/lenas_py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/shakhrayv/.virtualenvs/lenas_py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (1000 iteration) of 10 took 1029.561s\n",
      "  training loss (in-iteration): \t0.042329\n",
      "  validation loss (in-iteration): \t0.054550\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55648556044f4557ab02f67b513aaddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 (2000 iteration) of 10 took 1030.224s\n",
      "  training loss (in-iteration): \t0.041920\n",
      "  validation loss (in-iteration): \t0.053441\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e539dd1f4a4128aa79127776771077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 (3000 iteration) of 10 took 1049.521s\n",
      "  training loss (in-iteration): \t0.040988\n",
      "  validation loss (in-iteration): \t0.056013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409cf2d189794b3bb896a5c4b5e70261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 (4000 iteration) of 10 took 1030.356s\n",
      "  training loss (in-iteration): \t0.041912\n",
      "  validation loss (in-iteration): \t0.062443\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b656a60451d41fc83ba3ae963b04145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 (5000 iteration) of 10 took 1026.969s\n",
      "  training loss (in-iteration): \t0.043306\n",
      "  validation loss (in-iteration): \t0.056914\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a499f19b8d04f4ca1fb853d07d7f7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 (6000 iteration) of 10 took 1026.190s\n",
      "  training loss (in-iteration): \t0.041737\n",
      "  validation loss (in-iteration): \t0.055526\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c44298e99f84cc5b2d52d882f540987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 (7000 iteration) of 10 took 1028.156s\n",
      "  training loss (in-iteration): \t0.041424\n",
      "  validation loss (in-iteration): \t0.053422\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4958d6f673497aa3c167a9e722f11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 (8000 iteration) of 10 took 1030.074s\n",
      "  training loss (in-iteration): \t0.044824\n",
      "  validation loss (in-iteration): \t0.056381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fac04d1b60042549071466940971cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 (9000 iteration) of 10 took 1029.488s\n",
      "  training loss (in-iteration): \t0.042994\n",
      "  validation loss (in-iteration): \t0.055892\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ececf38af1428db1186eb20416b351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 (10000 iteration) of 10 took 1028.987s\n",
      "  training loss (in-iteration): \t0.040209\n",
      "  validation loss (in-iteration): \t0.057697\n"
     ]
    }
   ],
   "source": [
    "# epochs = 10\n",
    "steps_per_epoch = 1000\n",
    "\n",
    "batch_generator = get_batch_generator(data_aol, good_last_indeces_for_batch, batch_size=128)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    for iteration in tqdm_notebook(range(steps_per_epoch)):\n",
    "        \n",
    "        batch_queries = next(batch_generator)\n",
    "        if not len(batch_queries):\n",
    "            continue\n",
    "            \n",
    "        loss = compute_loss(model, batch_queries, return_logits=False)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        train_loss.append(loss.cpu().data.numpy())\n",
    "    \n",
    "\n",
    "    val_queries = get_val_batch_generator(data_aol_test, good_last_indeces_for_batch_test[:100000], \n",
    "                                          count_prev_good_indeces_limited_test,\n",
    "                                          batch_size=64, emb_size=300)\n",
    "    \n",
    "    count = 0\n",
    "    for batch_queries in val_queries:\n",
    "        if not len(batch_queries):\n",
    "            continue\n",
    "        loss = compute_loss(model, batch_queries, return_logits=False)\n",
    "        val_loss.append(loss.cpu().data.numpy())\n",
    "        count += 1\n",
    "    \n",
    "    print(\"Epoch {} ({} iteration) of {} took {:.3f}s\".format(\n",
    "        epoch + 1, (epoch + 1) * steps_per_epoch, epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-steps_per_epoch :])))\n",
    "    print(\"  validation loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(val_loss[-count:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лосс на валидации не улучшился, даже стало немного хуже :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeamSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем beam-search, который позволит не выбирать жадно наилучшее по вероятности следующее слово, а выбрать несколько наиболее вероятных предложений. Также позже в класс моделек был добавлен метод, позволяющий делать предсказание при помощи beam-search (по сути это небольшая модификация predict_beam_search, написанного здесь)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamNode(object):\n",
    "    def __init__(self, parent_node, cur_state, pred_word, cost):\n",
    "        super(BeamNode, self).__init__()\n",
    "        self.pred_word = pred_word\n",
    "        self.parent_node = parent_node # parent Node, None for root\n",
    "        self.cur_state = cur_state # recurrent layer hidden state\n",
    "        self.cum_cost = parent_node.cum_cost + cost if parent_node else cost # e.g. -log(p) of sequence up to current node (including)\n",
    "        self.length = 1 if parent_node is None else parent_node.length + 1\n",
    "        self._sequence = None\n",
    "    \n",
    "    def to_sequence(self):\n",
    "        # Return sequence of nodes from root to current node.\n",
    "        if not self._sequence:\n",
    "            self._sequence = []\n",
    "            current_node = self\n",
    "            while current_node:\n",
    "                self._sequence.append(current_node)\n",
    "                current_node = current_node.parent_node\n",
    "            self._sequence = self._sequence[::-1]\n",
    "        return self._sequence\n",
    "\n",
    "    def to_sequence_of_values(self):\n",
    "        return [int(s.pred_word.cpu().numpy()) for s in self.to_sequence()]\n",
    "    \n",
    "\n",
    "def predict_beam_search(self, previous_queries, current_words, beam_width=4, num_hypotheses=1):\n",
    "    batch_size = previous_queries.shape[0]\n",
    "    count_prev_words = current_words.shape[1]\n",
    "\n",
    "    queries_embeddings = []\n",
    "    for i in range(previous_queries.shape[1]):\n",
    "        queries_embeddings.append(self.query_level(previous_queries[:, i])[:,None])\n",
    "\n",
    "    queries_embeddings = torch.cat(queries_embeddings, dim=1)\n",
    "\n",
    "    session_states = rnn_loop_session(self.session_level, queries_embeddings)\n",
    "    session_last_state = session_states[-1]\n",
    "\n",
    "    hid_state = self.decoder.initial_state(session_last_state)\n",
    "\n",
    "    current_words = current_words.transpose(0,1)\n",
    "\n",
    "    #add bos in the begining\n",
    "    current_words = torch.cat([torch.ones((1, batch_size), dtype=torch.long).to(device) * bos_ix, current_words], dim=0)\n",
    "\n",
    "    all_logits = []\n",
    "    for x_t in current_words[:-1]:\n",
    "        hid_state, logits_next = self.decoder(self.query_level.embeddings(x_t), hid_state, \n",
    "                                              self.query_level.embeddings, queries_embeddings)\n",
    "    \n",
    "    \n",
    "    hypotheses_words = []\n",
    "    hypotheses = []\n",
    "    for i in range(batch_size):\n",
    "        words, nodes = beam_search(self, hid_state[i], current_words[-1,i], \n",
    "                                           queries_embeddings=queries_embeddings[i][None],\n",
    "                                           beam_width=beam_width, \n",
    "                                           num_hypotheses=num_hypotheses, \n",
    "                                           max_length=max_word_count - count_prev_words)\n",
    "        hypotheses_words.append(words)\n",
    "        hypotheses.append(nodes)\n",
    "        \n",
    "    return hypotheses_words, hypotheses\n",
    "\n",
    "\n",
    "def beam_search(self, init_state, init_word, queries_embeddings, beam_width=4, num_hypotheses=1, max_length=9):\n",
    "    next_fringe = [BeamNode(parent_node=None, cur_state=init_state, pred_word=init_word, cost=0.0)]\n",
    "    hypotheses = []\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "\n",
    "        fringe = []\n",
    "        for node in next_fringe:\n",
    "            if node.pred_word == eos_ix:\n",
    "                hypotheses.append(node)\n",
    "            else:\n",
    "                fringe.append(node)\n",
    "\n",
    "        if not fringe:\n",
    "            break\n",
    "        preds_words = torch.cat([node.pred_word[None] for node in fringe], dim=0)\n",
    "\n",
    "        hid_states = torch.stack([node.cur_state for node in fringe], dim=0)\n",
    "        \n",
    "        hid_state, logits_next = self.decoder(self.query_level.embeddings(preds_words), hid_states, \n",
    "                                              self.query_level.embeddings, \n",
    "                                              queries_embeddings.repeat(hid_states.shape[0],1,1))\n",
    "        \n",
    "        log_probs = nn.LogSoftmax()(logits_next)\n",
    "        log_probs_next, words_next = log_probs.topk(beam_width, dim=1, sorted=False)\n",
    "\n",
    "        next_fringe = []\n",
    "\n",
    "        for log_probs_next_n, words_next_n, hid_state_n, node_prev in zip(log_probs_next, words_next, \n",
    "                                                                          hid_state, fringe):\n",
    "\n",
    "            for word, log_prob in zip(words_next_n, log_probs_next_n):\n",
    "                new_node = BeamNode(parent_node=node_prev, cur_state=hid_state_n, \n",
    "                                    pred_word=word, cost=-log_prob)\n",
    "                if new_node.length == max_length + 1:\n",
    "                    hypotheses.append(new_node)\n",
    "                next_fringe.append(new_node)\n",
    "\n",
    "\n",
    "        next_fringe = sorted(next_fringe, key=lambda node: node.cum_cost)[:beam_width] # may move this into loop to save memory\n",
    "\n",
    "    hypotheses.sort(key=lambda node: node.cum_cost)\n",
    "    \n",
    "    return [end_node.to_sequence_of_values() for end_node in hypotheses[:num_hypotheses]], [end_node.to_sequence() for end_node in hypotheses[:num_hypotheses]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Делаем предсказние с помощью beam-search-а**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakhrayv/.virtualenvs/lenas_py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "val_queries = get_data_for_prediction(data_aol_test, good_last_indeces_for_batch_test[:10000], \n",
    "                                       count_prev_good_indeces_limited_test, count_words=1, \n",
    "                                       batch_size=32, shuffle=False)\n",
    "\n",
    "prev_queries, next_queries, answers_queries = next(val_queries)\n",
    "prev_queries = torch.LongTensor(prev_queries).to(device)\n",
    "next_queries = torch.LongTensor(next_queries).to(device)\n",
    "hypotheses, hyp_node = predict_beam_search(model, prev_queries, next_queries, beam_width=5, num_hypotheses=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['fort', 'worth', 'tx', 'irving', 'tx', 'area', 'hotels', 'EOS', 'PAD']\n"
     ]
    }
   ],
   "source": [
    "iii = 33\n",
    "\n",
    "for j in range(5):\n",
    "    print([indeces_to_words[word] for word in prev_queries.cpu().numpy()[iii][j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fort', 'worth', 'tx', 'irving', 'tx', 'area', 'hotels', 'EOS']\n",
      "['fort', 'new', 'tx', 'irving', 'tx', 'area', 'hotels', 'EOS']\n",
      "['fort', 'tx', 'worth', 'mu@@', 'tx', 'area', 'hotels', 'EOS']\n",
      "['fort', '18@@', 'tx', 'irving', 'tx', 'area', 'hotels', 'EOS']\n",
      "['fort', 'spac@@', 'tx', 'irving', 'tx', 'area', 'hotels', 'EOS']\n",
      "fort worth tx irving tx hotels\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print([indeces_to_words[next_queries.cpu().numpy()[iii][0]]] + [indeces_to_words[ind] for ind in np.array(hypotheses)[iii][i]][1:])\n",
    "print(answers_queries[iii])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итоговая лучшая модель с одним self-attention (без attention-а в декодере)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самой хорошей по качеству оказалась вторая модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelAttention(\n",
       "  (query_level): QueryLevelRNN(\n",
       "    (embeddings): Embedding(40157, 200)\n",
       "    (rnn): GRU(200, 1000)\n",
       "  )\n",
       "  (session_level): SessionLevelRNNCell(\n",
       "    (rnn_cell): GRUCell(1000, 1500)\n",
       "    (linear_attention): Linear(in_features=2500, out_features=500, bias=True)\n",
       "    (v_attention): Linear(in_features=500, out_features=1, bias=False)\n",
       "  )\n",
       "  (decoder): DecoderCell(\n",
       "    (init_transform): Linear(in_features=1500, out_features=1000, bias=True)\n",
       "    (gru_cell): GRUCell(200, 1000)\n",
       "    (transform_state): Linear(in_features=1000, out_features=200, bias=True)\n",
       "    (transform_word): Linear(in_features=200, out_features=200, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ModelAttention()\n",
    "model.load_state_dict(torch.load('trained_model_self_attention_0.025158_dict.pth'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на предсказания данной модели на тестовой выборки. Для этого будем рассматривать минимум 2 предыдущих запроса сессии и пытаться предсказать продолжение текущего запроса, если уже написано ровно 1 слово. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakhrayv/.virtualenvs/lenas_py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/shakhrayv/.virtualenvs/lenas_py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:129: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 642 ms, sys: 75.7 ms, total: 718 ms\n",
      "Wall time: 716 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# returns generator\n",
    "val_queries = get_data_for_prediction(data_aol_test, good_last_indeces_for_batch_test[:10000], \n",
    "                                       count_prev_good_indeces_limited_test, count_words=1, \n",
    "                                       batch_size=64, shuffle=False, min_count_prev_queries=2)\n",
    "\n",
    "prev_queries, next_queries, answers_queries = next(val_queries)\n",
    "prev_queries = torch.LongTensor(prev_queries).to(device)\n",
    "next_queries = torch.LongTensor(next_queries).to(device)\n",
    "hypotheses, hyp_node = model.predict_beam_search(prev_queries, next_queries, beam_width=5, num_hypotheses=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Примеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим на примеры предсказаний. Замечу, что все PREDICTION-ы начинаются на одно и то же слово, так как я предсказываю продолжение запроса, уже имея первой слово. ANSWER - настоящий запрос, который предсказываем. Также замечу, что PREDICTION это целых 5 отранжированных запросов, полученных при помощи beamsearch (можно и не 5 выбрать)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PREVIOUS QUERIES:\n",
      "broadway . vera . org \n",
      "broadway . vera . org \n",
      "vera . org \n",
      "\n",
      "     PREDICTION:\n",
      "broadway vera . org\n",
      "broadway vera\n",
      "broadway . org\n",
      "broadway . vera . org\n",
      "broadway pa@@ vera\n",
      "\n",
      "     ANSWER:\n",
      "broadway . vera . org\n"
     ]
    }
   ],
   "source": [
    "iii = 0\n",
    "print('     PREVIOUS QUERIES:')\n",
    "for j in range(5):\n",
    "    if prev_queries.cpu().numpy()[iii][j][0] != pad_ix:\n",
    "        q = np.array([indeces_to_words[word] for word in prev_queries.cpu().numpy()[iii][j]])\n",
    "        q = q[q != 'PAD']\n",
    "        print(' '.join(q)[:-3])\n",
    "    \n",
    "print('\\n     PREDICTION:')\n",
    "for i in range(5):\n",
    "    print(' '.join([indeces_to_words[next_queries.cpu().numpy()[iii][0]]] + [indeces_to_words[ind] for ind in np.array(hypotheses)[iii][i]][1:])[:-3])\n",
    "print(\"\\n     ANSWER:\")\n",
    "print(answers_queries[iii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PREVIOUS QUERIES:\n",
      "bath and body works \n",
      "edible oils \n",
      "edible oils \n",
      "\n",
      "     PREDICTION:\n",
      "body edible oils \n",
      "body oils \n",
      "body measure \n",
      "body oil expenses \n",
      "body and bath \n",
      "\n",
      "     ANSWER:\n",
      "body oils\n"
     ]
    }
   ],
   "source": [
    "iii = 2\n",
    "print('     PREVIOUS QUERIES:')\n",
    "for j in range(5):\n",
    "    if prev_queries.cpu().numpy()[iii][j][0] != pad_ix:\n",
    "        q = np.array([indeces_to_words[word] for word in prev_queries.cpu().numpy()[iii][j]])\n",
    "        q = q[q != 'PAD']\n",
    "        print(' '.join(q)[:-3])\n",
    "    \n",
    "print('\\n     PREDICTION:')\n",
    "for i in range(5):\n",
    "    print(' '.join([indeces_to_words[next_queries.cpu().numpy()[iii][0]]] + [indeces_to_words[ind] for ind in np.array(hypotheses)[iii][i]][1:])[:-3])\n",
    "print(\"\\n     ANSWER:\")\n",
    "print(answers_queries[iii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PREVIOUS QUERIES:\n",
      "free clip art \n",
      "free clip art \n",
      "free clip art \n",
      "sunset pictures \n",
      "sunset pictures \n",
      "\n",
      "     PREDICTION:\n",
      "black sunset pictures \n",
      "black sunset \n",
      "black sunset pics \n",
      "black art \n",
      "black sunset photos \n",
      "\n",
      "     ANSWER:\n",
      "black and white sunset photos\n"
     ]
    }
   ],
   "source": [
    "iii = 4\n",
    "print('     PREVIOUS QUERIES:')\n",
    "for j in range(5):\n",
    "    if prev_queries.cpu().numpy()[iii][j][0] != pad_ix:\n",
    "        q = np.array([indeces_to_words[word] for word in prev_queries.cpu().numpy()[iii][j]])\n",
    "        q = q[q != 'PAD']\n",
    "        print(' '.join(q)[:-3])\n",
    "    \n",
    "print('\\n     PREDICTION:')\n",
    "for i in range(5):\n",
    "    print(' '.join([indeces_to_words[next_queries.cpu().numpy()[iii][0]]] + [indeces_to_words[ind] for ind in np.array(hypotheses)[iii][i]][1:])[:-3])\n",
    "print(\"\\n     ANSWER:\")\n",
    "print(answers_queries[iii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PREVIOUS QUERIES:\n",
      "romantic photos \n",
      "romantic photos \n",
      "\n",
      "     PREDICTION:\n",
      "pictures romantic \n",
      "pictures romantic photos \n",
      "pictures romantic love \n",
      "pictures antonio \n",
      "pictures banks \n",
      "\n",
      "     ANSWER:\n",
      "pictures of fire\n"
     ]
    }
   ],
   "source": [
    "iii = 6\n",
    "print('     PREVIOUS QUERIES:')\n",
    "for j in range(3):\n",
    "    if prev_queries.cpu().numpy()[iii][j][0] != pad_ix:\n",
    "        q = np.array([indeces_to_words[word] for word in prev_queries.cpu().numpy()[iii][j]])\n",
    "        q = q[q != 'PAD']\n",
    "        print(' '.join(q)[:-3])\n",
    "    \n",
    "print('\\n     PREDICTION:')\n",
    "for i in range(5):\n",
    "    print(' '.join([indeces_to_words[next_queries.cpu().numpy()[iii][0]]] + [indeces_to_words[ind] for ind in np.array(hypotheses)[iii][i]][1:])[:-3])\n",
    "print(\"\\n     ANSWER:\")\n",
    "print(answers_queries[iii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PREVIOUS QUERIES:\n",
      "cbc companies \n",
      "cbc companies \n",
      "cbc companies \n",
      "national real estate settlement services \n",
      "national real estate settlement services \n",
      "\n",
      "     PREDICTION:\n",
      "pennsylvania real estate settlement services \n",
      "pennsylvania cbc companies \n",
      "pennsylvania real cbc companies \n",
      "pennsylvania real myspace settlement services\n",
      "pennsylvania real companies \n",
      "\n",
      "     ANSWER:\n",
      "pennsylvania real estate settlement services\n"
     ]
    }
   ],
   "source": [
    "iii = 12\n",
    "print('     PREVIOUS QUERIES:')\n",
    "for j in range(5):\n",
    "    if prev_queries.cpu().numpy()[iii][j][0] != pad_ix:\n",
    "        q = np.array([indeces_to_words[word] for word in prev_queries.cpu().numpy()[iii][j]])\n",
    "        q = q[q != 'PAD']\n",
    "        print(' '.join(q)[:-3])\n",
    "    \n",
    "print('\\n     PREDICTION:')\n",
    "for i in range(5):\n",
    "    print(' '.join([indeces_to_words[next_queries.cpu().numpy()[iii][0]]] + [indeces_to_words[ind] for ind in np.array(hypotheses)[iii][i]][1:])[:-3])\n",
    "print(\"\\n     ANSWER:\")\n",
    "print(answers_queries[iii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PREVIOUS QUERIES:\n",
      "live oak florida \n",
      "live oak florida \n",
      "live oak florida \n",
      "\n",
      "     PREDICTION:\n",
      "university live florida \n",
      "university oak florida \n",
      "university live oak \n",
      "university live oak florida  \n",
      "university murals florida \n",
      "\n",
      "     ANSWER:\n",
      "university of florida\n"
     ]
    }
   ],
   "source": [
    "iii = 21\n",
    "print('     PREVIOUS QUERIES:')\n",
    "for j in range(5):\n",
    "    if prev_queries.cpu().numpy()[iii][j][0] != pad_ix:\n",
    "        q = np.array([indeces_to_words[word] for word in prev_queries.cpu().numpy()[iii][j]])\n",
    "        q = q[q != 'PAD']\n",
    "        print(' '.join(q)[:-3])\n",
    "    \n",
    "print('\\n     PREDICTION:')\n",
    "for i in range(5):\n",
    "    print(' '.join([indeces_to_words[next_queries.cpu().numpy()[iii][0]]] + [indeces_to_words[ind] for ind in np.array(hypotheses)[iii][i]][1:])[:-3])\n",
    "print(\"\\n     ANSWER:\")\n",
    "print(answers_queries[iii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PREVIOUS QUERIES:\n",
      "movie theaters peru il \n",
      "peru illinois cen@@ ima \n",
      "peru illinois cinema \n",
      "\n",
      "     PREDICTION:\n",
      "peru illinois cinema \n",
      "peru illinois cinema phone \n",
      "peru illinois movie theaters \n",
      "peru illinois theaters \n",
      "peru theaters \n",
      "\n",
      "     ANSWER:\n",
      "peru illinois movie theater show@@ ings\n"
     ]
    }
   ],
   "source": [
    "iii = 22\n",
    "print('     PREVIOUS QUERIES:')\n",
    "for j in range(5):\n",
    "    if prev_queries.cpu().numpy()[iii][j][0] != pad_ix:\n",
    "        q = np.array([indeces_to_words[word] for word in prev_queries.cpu().numpy()[iii][j]])\n",
    "        q = q[q != 'PAD']\n",
    "        print(' '.join(q)[:-3])\n",
    "    \n",
    "print('\\n     PREDICTION:')\n",
    "for i in range(5):\n",
    "    print(' '.join([indeces_to_words[next_queries.cpu().numpy()[iii][0]]] + [indeces_to_words[ind] for ind in np.array(hypotheses)[iii][i]][1:])[:-3])\n",
    "print(\"\\n     ANSWER:\")\n",
    "print(answers_queries[iii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PREVIOUS QUERIES:\n",
      "jesse mccartney \n",
      "jesse mccartney \n",
      "\n",
      "     PREDICTION:\n",
      "pop@@ star mccartney \n",
      "pop@@ star jesse mccartney \n",
      "pop@@ mccartney\n",
      "pop@@ jesse mccartney \n",
      "pop@@ spiral mccartney\n",
      "\n",
      "     ANSWER:\n",
      "pop@@ star magazine . com\n"
     ]
    }
   ],
   "source": [
    "iii = 23\n",
    "print('     PREVIOUS QUERIES:')\n",
    "for j in range(2):\n",
    "    if prev_queries.cpu().numpy()[iii][j][0] != pad_ix:\n",
    "        q = np.array([indeces_to_words[word] for word in prev_queries.cpu().numpy()[iii][j]])\n",
    "        q = q[q != 'PAD']\n",
    "        print(' '.join(q)[:-3])\n",
    "    \n",
    "print('\\n     PREDICTION:')\n",
    "for i in range(5):\n",
    "    print(' '.join([indeces_to_words[next_queries.cpu().numpy()[iii][0]]] + [indeces_to_words[ind] for ind in np.array(hypotheses)[iii][i]][1:])[:-3])\n",
    "print(\"\\n     ANSWER:\")\n",
    "print(answers_queries[iii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PREVIOUS QUERIES:\n",
      "dry counties in florida \n",
      "williston florida \n",
      "williston florida \n",
      "nature coast hospital inc swb \n",
      "\n",
      "     PREDICTION:\n",
      "palatka williston florida\n",
      "palatka nature coast\n",
      "palatka nature and inc swb \n",
      "palatka in florida \n",
      "palatka nature coast hospital inc swb \n",
      "\n",
      "     ANSWER:\n",
      "palatka florida\n"
     ]
    }
   ],
   "source": [
    "iii = 26\n",
    "print('     PREVIOUS QUERIES:')\n",
    "for j in range(5):\n",
    "    if prev_queries.cpu().numpy()[iii][j][0] != pad_ix:\n",
    "        q = np.array([indeces_to_words[word] for word in prev_queries.cpu().numpy()[iii][j]])\n",
    "        q = q[q != 'PAD']\n",
    "        print(' '.join(q)[:-3])\n",
    "    \n",
    "print('\\n     PREDICTION:')\n",
    "for i in range(5):\n",
    "    print(' '.join([indeces_to_words[next_queries.cpu().numpy()[iii][0]]] + [indeces_to_words[ind] for ind in np.array(hypotheses)[iii][i]][1:])[:-3])\n",
    "print(\"\\n     ANSWER:\")\n",
    "print(answers_queries[iii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PREVIOUS QUERIES:\n",
      "williston florida \n",
      "williston florida \n",
      "nature coast hospital inc swb \n",
      "palatka florida \n",
      "florida counties map \n",
      "\n",
      "     PREDICTION:\n",
      "lake florida \n",
      "lake florida map \n",
      "lake palatka \n",
      "lake coast \n",
      "lake florida counties map \n",
      "\n",
      "     ANSWER:\n",
      "lake city florida\n"
     ]
    }
   ],
   "source": [
    "iii = 28\n",
    "print('     PREVIOUS QUERIES:')\n",
    "for j in range(5):\n",
    "    if prev_queries.cpu().numpy()[iii][j][0] != pad_ix:\n",
    "        q = np.array([indeces_to_words[word] for word in prev_queries.cpu().numpy()[iii][j]])\n",
    "        q = q[q != 'PAD']\n",
    "        print(' '.join(q)[:-3])\n",
    "    \n",
    "print('\\n     PREDICTION:')\n",
    "for i in range(5):\n",
    "    print(' '.join([indeces_to_words[next_queries.cpu().numpy()[iii][0]]] + [indeces_to_words[ind] for ind in np.array(hypotheses)[iii][i]][1:])[:-3])\n",
    "print(\"\\n     ANSWER:\")\n",
    "print(answers_queries[iii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем метрику : перплексию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, queries, vocab_len=len(all_words), return_logits=False, **flags):\n",
    "    targets = torch.LongTensor(queries).to(device)\n",
    "    queries = torch.LongTensor(queries).to(device)\n",
    "    batchs_size, queries_count, time = queries.shape\n",
    "    \n",
    "        \n",
    "    targets = torch.cat((targets, torch.ones(batchs_size, queries_count, 1, dtype=torch.long).to(device) * eos_ix), dim=2)\n",
    "    #delete first query (we won't predict it)\n",
    "    targets = targets[:,1:].contiguous().view(-1,)\n",
    "    logits = model(queries).contiguous().view(-1, vocab_len)\n",
    "    \n",
    "#     log_probs = nn.LogSoftmax()(logits)\n",
    "#     log_probs = log_probs[targets != pad_ix]\n",
    "#     targets = targets[targets != pad_ix]\n",
    "#     arange = torch.arange(log_probs.shape[0])\n",
    "#     perplexity = torch.exp(-torch.mean(log_probs[arange, targets])).cpu().detach().numpy()\n",
    "    return torch.exp(criterion(logits, targets))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakhrayv/.virtualenvs/lenas_py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0357462439100795\n"
     ]
    }
   ],
   "source": [
    "val_queries = get_val_batch_generator(data_aol_test, good_last_indeces_for_batch_test, \n",
    "                                          count_prev_good_indeces_limited_test,\n",
    "                                          batch_size=64, emb_size=300)\n",
    "    \n",
    "count = 0\n",
    "mean_perplexity = 0\n",
    "for batch_queries in val_queries:\n",
    "    if not len(batch_queries):\n",
    "        continue\n",
    "    perplexity = compute_perplexity(model, batch_queries, return_logits=False)\n",
    "    mean_perplexity += (perplexity.cpu().data.numpy())\n",
    "    count += 1\n",
    "    \n",
    "print(mean_perplexity / count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним перплексию с обычными n-gram-ыми моделями и n-gram-ой моделью с сглаживанием Кнейзера-Нея."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n = 2</td>\n",
       "      <td>2.327883e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n = 3</td>\n",
       "      <td>7.629578e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kneser-Ney smooting, n = 3</td>\n",
       "      <td>1.892584e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neural model with self-attention</td>\n",
       "      <td>1.035746e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model    perplexity\n",
       "0                             n = 2  2.327883e+08\n",
       "1                             n = 3  7.629578e+14\n",
       "2        Kneser-Ney smooting, n = 3  1.892584e+05\n",
       "3  Neural model with self-attention  1.035746e+00"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df['model'] = ['n = 2', 'n = 3', 'Kneser-Ney smooting, n = 3', 'Neural model with self-attention']\n",
    "df['perplexity'] = [232788250.5037612, 762957792024751.1, 189258.402469, mean_perplexity / count]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO-DO \n",
    "Find way to count MRR metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model in Tensorflow\n",
    "Попытки написать модель на tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logits(model, queries, vocab_emb_matrix, **flags):\n",
    "    \"\"\"\n",
    "    :param inp: input tokens matrix, int32[batch, time]\n",
    "    :param out: reference tokens matrix, int32[batch, time]\n",
    "    :returns: logits of shape [batch, time, voc_size]\n",
    "    \n",
    "    * logits must be a linear output of your neural network.\n",
    "    * logits [:, 0, :] should always predic BOS\n",
    "    * logits [:, -1, :] should be probabilities of last token in out\n",
    "    This function should NOT return logits predicted when taking out[:, -1] as y_prev\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(queries)[0]\n",
    "    \n",
    "    inp = crop(1, 0, max_query_sequence_len - 1)(queries)\n",
    "    out = crop(1, 1, max_query_sequence_len)(queries)\n",
    "\n",
    "    queries_emb = model.encode_query(inp)\n",
    "    \n",
    "    first_decode_state = model.encode_session(queries_emb)\n",
    "#     first_decode_state = L.Reshape(target_shape=(max_query_sequence_len - 1, hid_size_query))(first_decode_state)\n",
    "    \n",
    "    # initial logits: always predict BOS\n",
    "    first_logits = tf.log(tf.one_hot(tf.fill([batch_size], bos_ix), vocab_size) + 1e-30)\n",
    "    output_logits = first_logits\n",
    "    # Decode step\n",
    "\n",
    "    # You can now use tf.scan to run step several times.\n",
    "    # use tf.transpose(out) as elems (to process one time-step at a time)\n",
    "    # docs: https://www.tensorflow.org/api_docs/python/tf/scan\n",
    "    \n",
    "    def step(blob, y_prev):\n",
    "        # Given previous state, obtain next state and next token logits\n",
    "        \n",
    "        prev_state = blob\n",
    "        next_state = model.decode(prev_state, y_prev)\n",
    "\n",
    "#         output_logits = dense_logits(L.Concatenate()([next_state, y_prev]))   # not sure here\n",
    "#         output_logits = tf.tensordot(output_logits, tf.transpose(vocab_emb_matrix), axes=1)\n",
    "#         output_logits = tf.log(tf.one_hot(tf.fill([batch_size], bos_ix), vocab_size) + 1e-30)\n",
    "#         query_logits.append(output_logits)\n",
    "        \n",
    "        return next_state\n",
    "    \n",
    "    all_queries_logits = []\n",
    "    \n",
    "    for i in range(max_query_sequence_len - 1):\n",
    "\n",
    "        first_state = tf.squeeze(crop(1, i, i + 1)(first_decode_state), axis=1)\n",
    "        out_i = tf.transpose(tf.squeeze(crop(1, i, i + 1)(out), axis=1), perm=(1, 0, 2))\n",
    "        states_seq = tf.scan(step,\n",
    "                elems=out_i,\n",
    "               initializer=first_state)\n",
    "\n",
    "        concat_states_queries = tf.concat([states_seq, out_i], axis=-1)\n",
    "        concat_states_queries = tf.transpose(concat_states_queries, perm=(1, 0, 2))\n",
    "        logits = (L.TimeDistributed(model.dense_logits)(concat_states_queries))\n",
    "        \n",
    "        logits = L.Reshape(target_shape=(max_word_count, emb_size))(logits)\n",
    "        layer = L.Lambda(lambda x: tf.tensordot(x, tf.transpose(vocab_emb_matrix), axes=1))\n",
    "        \n",
    "        logits_seq = L.Reshape(target_shape=(max_word_count, vocab_size))(L.TimeDistributed(layer)(logits))\n",
    "        logits_seq = tf.transpose(logits_seq, perm=(1, 0, 2))\n",
    "\n",
    "        logits_seq = tf.concat([first_logits[None], logits_seq], axis=0)[:-1]\n",
    "        logits_seq = tf.transpose(logits_seq, perm=(1, 0, 2))\n",
    "        all_queries_logits.append(L.Reshape(target_shape=(1, max_word_count, vocab_size))(logits_seq))\n",
    "\n",
    "    concat_all_logits = tf.concat(all_queries_logits, axis=1)\n",
    "    return concat_all_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.layers as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(dimension, start, end):\n",
    "    # Crops (or slices) a Tensor on a given dimension from start to end\n",
    "    # example : to crop tensor x[:, :, 5:10]\n",
    "    # call slice(2, 5, 10) as you want to crop on the second dimension\n",
    "    def func(x):\n",
    "        if dimension == 0:\n",
    "            return x[start: end]\n",
    "        if dimension == 1:\n",
    "            return x[:, start: end]\n",
    "        if dimension == 2:\n",
    "            return x[:, :, start: end]\n",
    "        if dimension == 3:\n",
    "            return x[:, :, :, start: end]\n",
    "        if dimension == 4:\n",
    "            return x[:, :, :, :, start: end]\n",
    "    return L.Lambda(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModelNew:\n",
    "    def __init__(self, name='model', emb_size=300, hid_size_query=1000, hid_size_session=1500):\n",
    "        self.emb_size = emb_size\n",
    "        self.hid_size_query = hid_size_query\n",
    "        self.hid_size_session = hid_size_session\n",
    "        \n",
    "        \n",
    "        self.query_level_rnn = L.GRU(hid_size_query)\n",
    "        self.session_level_rnn = L.GRU(hid_size_session, return_state=True, return_sequences=True)\n",
    "        self.decoding_rnn = tf.nn.rnn_cell.GRUCell(hid_size_query)\n",
    "        self.dec_start = L.Dense(hid_size_query, input_shape=(hid_size_session,), activation='tanh')\n",
    "        self.dense_logits = L.Dense(emb_size, input_shape=(self.hid_size_query + emb_size,))\n",
    "        \n",
    "        #input data\n",
    "#         self.input = tf.placeholder('float32', [None, max_query_sequence_len, max_word_count, emb_size])\n",
    "#         self.input_queries = crop(1, 0, max_query_sequence_len - 1)(self.input)\n",
    "#         self.output_queries = crop(1, 1, max_query_sequence_len)(self.input)\n",
    "        \n",
    "#         self.vocab_emb_matrix = tf.placeholder('float32', [vocab_size, emb_size])\n",
    "        \n",
    "# #         self.prev_tokens = tf.placeholder('float32', [None, emb_size])\n",
    "# #         self.prev_decode_state = tf.placeholder('float32', [None, hid_size_query])\n",
    "        \n",
    "#         self.targets = tf.placeholder('float32', [None, max_query_sequence_len - 1,  None, vocab_size])\n",
    "        \n",
    "        \n",
    "#         self.emb_queries = self.encode_query(self.input_queries)\n",
    "#         self.init_decode_states = self.encode_session(self.emb_queries)\n",
    "        \n",
    "# #         self.next_state, self.next_logits = self.decode(self.prev_decode_state, self.prev_tokens)  \n",
    "        \n",
    "        self.weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        \n",
    "    def encode_query(self, inp_queries):\n",
    "        all_last_states = []\n",
    "        with tf.variable_scope('enc0'):\n",
    "            for i in range(max_query_sequence_len - 1):\n",
    "                sliced_inp = tf.squeeze(crop(1, i, i + 1)(inp_queries), axis=1)\n",
    "                last_state = self.query_level_rnn(sliced_inp)\n",
    "                all_last_states.append(L.Reshape(target_shape=(1, self.hid_size_query))(last_state))\n",
    "                \n",
    "        concat_all_states = L.Concatenate(axis=1)(all_last_states)\n",
    "        return concat_all_states\n",
    "        \n",
    "    def encode_session(self, inp_queries):\n",
    "        seq_states, last_state = self.session_level_rnn(inp_queries)\n",
    "        decode_start_state = L.TimeDistributed(self.dec_start)(seq_states)\n",
    "#         print('decode_start_state', decode_start_state)\n",
    "        return decode_start_state\n",
    "        \n",
    "        \n",
    "    def decode(self, prev_state, prev_tokens):\n",
    "#         with tf.variable_scope('dec0'):\n",
    "#             prev_state_i = tf.squeeze(crop(1, i, i + 1)(prev_state), axis=1)\n",
    "#             prev_tokens_i = tf.squeeze(crop(1, i, i + 1)(prev_tokens), axis=1)\n",
    "#             print('-- ', prev_state_i.shape, prev_tokens_i.shape)\n",
    "        new_dec_out, new_dec_state = self.decoding_rnn(prev_tokens, prev_state)\n",
    "\n",
    "#             output_logits = self.logits(L.Concatenate()([new_dec_out, prev_tokens]))   # not sure here\n",
    "#             output_logits = tf.tensordot(output_logits, tf.transpose(self.vocab_emb_matrix), axes=1)\n",
    "#         output_logits = self.logits(new_dec_state)\n",
    "\n",
    "        return new_dec_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# индекс слова bos - begin of sentence\n",
    "def compute_logits(model, queries, vocab_emb_matrix, **flags):\n",
    "    \"\"\"\n",
    "    :param inp: input tokens matrix, int32[batch, time]\n",
    "    :param out: reference tokens matrix, int32[batch, time]\n",
    "    :returns: logits of shape [batch, time, voc_size]\n",
    "    \n",
    "    * logits must be a linear output of your neural network.\n",
    "    * logits [:, 0, :] should always predic BOS\n",
    "    * logits [:, -1, :] should be probabilities of last token in out\n",
    "    This function should NOT return logits predicted when taking out[:, -1] as y_prev\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(queries)[0]\n",
    "    \n",
    "    inp = crop(1, 0, max_query_sequence_len - 1)(queries)\n",
    "    out = crop(1, 1, max_query_sequence_len)(queries)\n",
    "\n",
    "    queries_emb = model.encode_query(inp)\n",
    "    \n",
    "    first_decode_state = model.encode_session(queries_emb)\n",
    "#     first_decode_state = L.Reshape(target_shape=(max_query_sequence_len - 1, hid_size_query))(first_decode_state)\n",
    "    \n",
    "    # initial logits: always predict BOS\n",
    "    first_logits = tf.log(tf.one_hot(tf.fill([batch_size], bos_ix), vocab_size) + 1e-30)\n",
    "    output_logits = first_logits\n",
    "    # Decode step\n",
    "\n",
    "    # You can now use tf.scan to run step several times.\n",
    "    # use tf.transpose(out) as elems (to process one time-step at a time)\n",
    "    # docs: https://www.tensorflow.org/api_docs/python/tf/scan\n",
    "    \n",
    "    def step(blob, y_prev):\n",
    "        # Given previous state, obtain next state and next token logits\n",
    "        \n",
    "        prev_state = blob\n",
    "        next_state = model.decode(prev_state, y_prev)\n",
    "\n",
    "#         output_logits = dense_logits(L.Concatenate()([next_state, y_prev]))   # not sure here\n",
    "#         output_logits = tf.tensordot(output_logits, tf.transpose(vocab_emb_matrix), axes=1)\n",
    "#         output_logits = tf.log(tf.one_hot(tf.fill([batch_size], bos_ix), vocab_size) + 1e-30)\n",
    "#         query_logits.append(output_logits)\n",
    "        \n",
    "        return next_state\n",
    "    \n",
    "    all_queries_logits = []\n",
    "    \n",
    "    for i in range(max_query_sequence_len - 1):\n",
    "\n",
    "        first_state = tf.squeeze(crop(1, i, i + 1)(first_decode_state), axis=1)\n",
    "        out_i = tf.transpose(tf.squeeze(crop(1, i, i + 1)(out), axis=1), perm=(1, 0, 2))\n",
    "        states_seq = tf.scan(step,\n",
    "                elems=out_i,\n",
    "               initializer=first_state)\n",
    "\n",
    "        concat_states_queries = tf.concat([states_seq, out_i], axis=-1)\n",
    "        concat_states_queries = tf.transpose(concat_states_queries, perm=(1, 0, 2))\n",
    "        logits = (L.TimeDistributed(model.dense_logits)(concat_states_queries))\n",
    "        \n",
    "        logits = L.Reshape(target_shape=(max_word_count, emb_size))(logits)\n",
    "        layer = L.Lambda(lambda x: tf.tensordot(x, tf.transpose(vocab_emb_matrix), axes=1))\n",
    "        \n",
    "        logits_seq = L.Reshape(target_shape=(max_word_count, vocab_size))(L.TimeDistributed(layer)(logits))\n",
    "        logits_seq = tf.transpose(logits_seq, perm=(1, 0, 2))\n",
    "\n",
    "        logits_seq = tf.concat([first_logits[None], logits_seq], axis=0)[:-1]\n",
    "        logits_seq = tf.transpose(logits_seq, perm=(1, 0, 2))\n",
    "        all_queries_logits.append(L.Reshape(target_shape=(1, max_word_count, vocab_size))(logits_seq))\n",
    "\n",
    "    concat_all_logits = tf.concat(all_queries_logits, axis=1)\n",
    "    return concat_all_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elkir/.virtualenvs/py/lib/python3.7/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "model = BasicModelNew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dummy_queries = tf.constant(np.ones((15, max_query_sequence_len, max_word_count, emb_size)) * 0.1, dtype='float32')\n",
    "\n",
    "dummy_vocab_emb_matrix = tf.constant(np.ones((445479, 300)) * 0.1, dtype='float32')\n",
    "\n",
    "logits_computed = compute_logits(model, dummy_queries, dummy_vocab_emb_matrix)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "dummy_logits = sess.run(logits_computed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_length(seq, eos_ix, dtype=tf.int32):\n",
    "    \"\"\" Compute length given output indices and eos code \"\"\"\n",
    "    is_eos = tf.cast(tf.equal(seq, eos_ix), dtype)\n",
    "    count_eos = tf.cumsum(is_eos,axis=2, exclusive=True)\n",
    "    lengths = tf.reduce_sum(tf.cast(tf.equal(count_eos, 0), dtype), axis=2)\n",
    "    return lengths\n",
    "\n",
    "\n",
    "def infer_mask(seq, eos_ix, dtype=tf.float32):\n",
    "    lengths = infer_length(seq, eos_ix)\n",
    "    mask = tf.sequence_mask(lengths, maxlen=tf.shape(seq)[2], dtype=dtype)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def select_values_over_last_axis(values, indices):\n",
    "    \"\"\" Auxiliary function to select logits corresponding to chosen tokens.\"\"\"\n",
    "\n",
    "    batch_size, count_out_queries, count_words = tf.shape(indices)[0], tf.shape(indices)[1], tf.shape(indices)[2]\n",
    "    \n",
    "    batch_i = tf.tile(tf.range(0,batch_size)[:, None][:, None],[1,count_out_queries, count_words])\n",
    "    queries_i = tf.tile(tf.range(0, count_out_queries)[:,None][None],[batch_size, 1, count_words])\n",
    "    time_i = tf.tile(tf.range(0, count_words)[None,:][None, :], [batch_size, count_out_queries,1])\n",
    "    indices_nd = tf.stack([batch_i, queries_i, time_i, indices], axis=-1)\n",
    "    return tf.gather_nd(values, indices_nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, queries, vocab_emb_matrix, targets, **flags):\n",
    "    \"\"\" Compute loss (float32 scalar) as in the formula above \"\"\"\n",
    "    targets = crop(1, 1, max_query_sequence_len)(targets)\n",
    "    mask = infer_mask(targets, eos_ix)   \n",
    "    \n",
    "    logits_seq = compute_logits(model, queries, vocab_emb_matrix, **flags)\n",
    "    \n",
    "    probs_out = select_values_over_last_axis(tf.nn.log_softmax(logits_seq, axis=-1), targets)\n",
    "    loss = -tf.reduce_sum(probs_out * mask) / tf.reduce_sum(mask)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "model = BasicModelNew()\n",
    "\n",
    "dummy_queries = tf.constant(np.ones((15, max_query_sequence_len, max_word_count, emb_size)) * 0.1, dtype='float32')\n",
    "dummy_vocab_emb_matrix = tf.constant(np.ones((445479, 300)) * 0.1, dtype='float32')\n",
    "\n",
    "targets = tf.random.uniform((15, max_query_sequence_len, max_word_count), 0, vocab_size, dtype=tf.dtypes.int32)\n",
    "\n",
    "dummy_loss = compute_loss(model, dummy_queries, dummy_vocab_emb_matrix, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 19.236992\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "dummy_loss = sess.run(dummy_loss)\n",
    "print(\"Loss:\", dummy_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_logits = self.logits(L.Concatenate()([new_dec_out, prev_tokens]))   # not sure here\n",
    "#             output_logits = tf.tensordot(output_logits, tf.transpose(self.vocab_emb_matrix), axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elkir/.virtualenvs/py/lib/python3.7/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/elkir/.virtualenvs/py/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "model = BasicModelNew()\n",
    "\n",
    "\n",
    "vocab_matrix = tf.placeholder('float32', [vocab_size, emb_size])\n",
    "queries = tf.placeholder('float32', [None, max_query_sequence_len, max_word_count, emb_size])\n",
    "targets = tf.placeholder('int32', [None, max_query_sequence_len, max_word_count])\n",
    "\n",
    "## need it for testing time\n",
    "prev_tokens = tf.placeholder('float32', [None, model.emb_size])\n",
    "prev_state = tf.placeholder('float32', [None, model.hid_size_query])\n",
    "emb_queries = model.encode_query(crop(1, 0, max_query_sequence_len - 1)(queries))\n",
    "init_decode_states = model.encode_session(emb_queries)\n",
    "next_state = model.decode(prev_state, prev_tokens)  \n",
    "next_logits = model.dense_logits(L.Concatenate()([next_state, prev_tokens]))\n",
    "next_logits = tf.tensordot(next_logits, tf.transpose(vocab_matrix), axes=1)\n",
    "##\n",
    "\n",
    "loss = compute_loss(model, queries, vocab_matrix, targets)\n",
    "logits = compute_logits(model, queries, vocab_matrix)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "metrics = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_answer(numbers):\n",
    "    for i in range(len(numbers)):\n",
    "        s = ''\n",
    "        for j in range(len(numbers[i])):\n",
    "            s += ' ' + indeces_to_words[numbers[i][j]]\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BOS EOS EOS EOS EOS EOS EOS EOS EOS\n",
      " BOS bmg EOS EOS EOS EOS EOS EOS EOS\n",
      " BOS harlequin EOS EOS EOS EOS EOS EOS EOS\n",
      " BOS myfloridacounty EOS EOS EOS EOS EOS EOS EOS\n",
      " BOS home remedies for hair growth EOS EOS EOS\n",
      " BOS home remedies for hair growth EOS EOS EOS\n"
     ]
    }
   ],
   "source": [
    "print_answer(val_targets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BOS nsa EOS EOS EOS EOS EOS EOS EOS\n",
      " BOS nsa EOS EOS EOS EOS EOS EOS EOS\n",
      " BOS nsa EOS EOS EOS EOS EOS EOS EOS\n",
      " BOS nsa EOS EOS EOS EOS EOS EOS EOS\n",
      " BOS nsa EOS EOS EOS EOS EOS EOS EOS\n"
     ]
    }
   ],
   "source": [
    "print_answer(np.argmax(all_val_logits[-1][10], axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss=7.501\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-863-46e455f32423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                      vocab_matrix: vocab_emb_matrix}\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = 100\n",
    "\n",
    "batch_generator = get_batch(data_aol, good_last_indeces_for_batch[:10000], batch_size=15)\n",
    "val_queries, val_targets = get_one_batch(data_aol, good_last_indeces_for_batch[10000:11000])\n",
    "\n",
    "feed_dict_validation = {queries: val_queries, \n",
    "                        targets: val_targets, \n",
    "                        vocab_matrix: vocab_emb_matrix}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for iteration in range(steps_per_epoch):\n",
    "        step = len(metrics['train_loss']) + 1\n",
    "        \n",
    "        batch_queries, batch_targets = next(batch_generator)\n",
    "        \n",
    "        feed_dict = {queries: batch_queries, \n",
    "                     targets: batch_targets, \n",
    "                     vocab_matrix: vocab_emb_matrix}\n",
    "        \n",
    "        loss_train, _ = sess.run([loss, train_step], feed_dict)\n",
    "        \n",
    "        metrics['train_loss'].append([step, loss_train])\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            val_loss = sess.run(loss, feed_dict_validation)\n",
    "            metrics['val_loss'].append([step, val_loss])\n",
    "            \n",
    "            clear_output(True)\n",
    "            plt.figure(figsize=(12,4))\n",
    "            for i, (name, history) in enumerate(sorted(metrics.items())):\n",
    "                plt.subplot(1, len(metrics), i + 1)\n",
    "                plt.title(name)\n",
    "                plt.plot(*zip(*history))\n",
    "                plt.grid()\n",
    "            plt.show();\n",
    "            \n",
    "            print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_data):\n",
    "    test_queries = test_queries_to_matrices(test_data)\n",
    "    feed_dict = {queries: test_queries, \n",
    "                 vocab_matrix: vocab_emb_matrix}\n",
    "\n",
    "#     print(test_queries)\n",
    "    predicted_tokens = [[bos_ix for _ in range(len(test_data))]]\n",
    "    \n",
    "    states = sess.run(init_decode_states, feed_dict)\n",
    "    tokens = [np.ones(300) for _ in range(len(test_data))] #BOS words\n",
    "    state = states[:,-1]\n",
    "    \n",
    "    for t in range(max_word_count):\n",
    "        f_dict = {prev_state : state, prev_tokens : tokens, vocab_matrix: vocab_emb_matrix}\n",
    "        state, logits = sess.run([next_state, next_logits], f_dict)\n",
    "#         print('state, logits', state.shape, logits.shape)\n",
    "        \n",
    "        next_number_tokens = np.argmax(logits, axis=-1)\n",
    "        predicted_tokens.append(next_number_tokens)\n",
    "        \n",
    "        tokens = [model_wv[indeces_to_words[id_word]] for id_word in next_number_tokens]\n",
    "        \n",
    "    print(np.array(predicted_tokens).shape)\n",
    "    return predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss=9.942\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-655-46e455f32423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                      vocab_matrix: vocab_emb_matrix}\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = 100\n",
    "\n",
    "batch_generator = get_batch(data_aol, good_last_indeces_for_batch[:10000], batch_size=15)\n",
    "val_queries, val_targets = get_one_batch(data_aol, good_last_indeces_for_batch[10000:11000])\n",
    "\n",
    "feed_dict_validation = {queries: val_queries, \n",
    "                        targets: val_targets, \n",
    "                        vocab_matrix: vocab_emb_matrix}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for iteration in range(steps_per_epoch):\n",
    "        step = len(metrics['train_loss']) + 1\n",
    "        \n",
    "        batch_queries, batch_targets = next(batch_generator)\n",
    "        \n",
    "        feed_dict = {queries: batch_queries, \n",
    "                     targets: batch_targets, \n",
    "                     vocab_matrix: vocab_emb_matrix}\n",
    "        \n",
    "        loss_train, _ = sess.run([loss, train_step], feed_dict)\n",
    "        \n",
    "        metrics['train_loss'].append([step, loss_train])\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            val_loss = sess.run(loss, feed_dict_validation)\n",
    "            metrics['val_loss'].append([step, val_loss])\n",
    "            \n",
    "            clear_output(True)\n",
    "            plt.figure(figsize=(12,4))\n",
    "            for i, (name, history) in enumerate(sorted(metrics.items())):\n",
    "                plt.subplot(1, len(metrics), i + 1)\n",
    "                plt.title(name)\n",
    "                plt.plot(*zip(*history))\n",
    "                plt.grid()\n",
    "            plt.show();\n",
    "            \n",
    "            print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
